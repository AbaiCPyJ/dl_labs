{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most recent version of this notebook is available at https://github.com/nadiinchi/dl_labs/blob/master/lab_attention.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "import numpy as np\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def get_scores(self, features, queries):\n",
    "        \"\"\"\n",
    "        features: [batch_size x num_objects x obj_feature_dim]\n",
    "        queries:  [batch_size x num_queries x query_feature_dim]\n",
    "        Returns matrix of scores with shape [batch_size x num_queries x num_objects].\n",
    "        \"\"\"\n",
    "        raise NotImplementedError()                \n",
    "\n",
    "    def attend(self, features, queries, mask=None):\n",
    "        \"\"\"\n",
    "        features:        [batch_size x num_objects x obj_feature_dim]\n",
    "        queries:         [batch_size x num_queries x query_feature_dim]\n",
    "        mask, optional:  [batch_size x num_queries x num_objects]\n",
    "        Returns matrix of features for queries with shape [batch_size x num_queries x obj_feature_dim].\n",
    "        If mask is not None, set corresponding to mask weights to zero.\n",
    "        Saves detached weights as self.last_weights for further visualization.\n",
    "        \"\"\"\n",
    "        # your code here\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdditiveAttention(Attention):\n",
    "    \"\"\"\n",
    "    Bahdanau et al. \"Neural Machine Translation by Jointly Learning to Align and Translate\", 2014.\n",
    "    \"\"\"\n",
    "    def __init__(self, obj_feature_dim, query_feature_dim, hidden_dim):\n",
    "        \"\"\"\n",
    "        obj_feature_dim   - dimensionality of attention object features vector\n",
    "        query_feature_dim - dimensionality of attention query vector\n",
    "        hidden_dim        - dimensionality of latent vectors of attention \n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        # your code here\n",
    "\n",
    "    def get_scores(self, features, queries):\n",
    "        \"\"\"\n",
    "        features: [batch_size x num_objects x obj_feature_dim]\n",
    "        queries:  [batch_size x num_queries x query_feature_dim]\n",
    "        Returns matrix of scores with shape [batch_size x num_queries x num_objects].\n",
    "        \"\"\"\n",
    "        # your code here\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiplicativeAttention(Attention):\n",
    "    \"\"\"\n",
    "    Luong et al. \"Effective approaches to attention-based neural machine translation\", 2015.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def get_scores(self, features, queries):\n",
    "        \"\"\"\n",
    "        features: [batch_size x num_objects x feature_dim]\n",
    "        queries:  [batch_size x num_queries x feature_dim]\n",
    "        Returns matrix of scores with shape [batch_size x num_queries x num_objects].\n",
    "        \"\"\"\n",
    "        # your code here\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScaledDotProductAttention(Attention):\n",
    "    \"\"\"\n",
    "    Vaswani et al. \"Attention Is All You Need\", 2017.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def get_scores(self, features, queries):\n",
    "        \"\"\"\n",
    "        features: [batch_size x num_objects x feature_dim]\n",
    "        queries:  [batch_size x num_queries x feature_dim]\n",
    "        Returns matrix of scores with shape [batch_size x num_queries x num_objects].\n",
    "        \"\"\"\n",
    "        # your code here\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# time to check that your attention works\n",
    "# your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perm_generator(batch_size, perm_size):\n",
    "    \"\"\"\n",
    "    Generates batch of batch_size objects.\n",
    "    Each object consists of two random permutations with length perm_size.\n",
    "    The target for the object is the product of its two permutations.\n",
    "    \"\"\"\n",
    "    # your code here\n",
    "    return objects, correct_answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# time to check your generator\n",
    "# your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoder(nn.Module):\n",
    "    def __init__(self, dim, max_len=50, scale=10000.0, add=True):\n",
    "        \"\"\"\n",
    "        Transforms input as described by Vaswani et al. in \"Attention Is All You Need\", 2017.\n",
    "        dim     - dimension of positional embeddings.\n",
    "        max_len - maximal length of sequence, for precomputing\n",
    "        scale   - scale factor for frequency for positional embeddings\n",
    "        add     - boolean, if add is False, concatenate positional embeddings with input instead of adding\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        \n",
    "        self.dim = dim\n",
    "        self.add = add\n",
    "        if add:\n",
    "            self.extra_output_shape = 0\n",
    "        else:\n",
    "            self.extra_output_shape = dim\n",
    "\n",
    "        # your code here\n",
    "               \n",
    "    def forward(self, input):\n",
    "        \"\"\"\n",
    "        input - [batch_size x sequence_len x features_dim]\n",
    "        If self.add is True, self.dim = featurs_dim.\n",
    "        Returns input with added or concatenated positional embeddings (depending on self.add).\n",
    "        \"\"\"\n",
    "        # your code here\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# time to draw positional encoder\n",
    "# your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_autoregressive_mask(size):\n",
    "    \"\"\"\n",
    "    Returns attention mask of given size for autoregressive model.\n",
    "    \"\"\"\n",
    "    # your code here\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PermMultiplier(nn.Module):\n",
    "    def __init__(self, perm_len, embedding_dim, hidden_dim, attention, pos_enc, autoregressive):\n",
    "        \"\"\"\n",
    "        perm_len       - permutation length (the input is twice longer)\n",
    "        embedding_dim  - dimensionality of integer embeddings\n",
    "        hidden_dim     - dimensionality of LSTM output\n",
    "        attention      - Attention object\n",
    "        pos_enc        - PositionalEncoder object or None\n",
    "        autoregressive - boolean, if True, then model must use autoregressive mask for attention\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.autoregressive = autoregressive\n",
    "        self.perm_len = perm_len\n",
    "        # your code here\n",
    "\n",
    "    def forward(self, input):\n",
    "        \"\"\"\n",
    "        Perform forward pass through layers:\n",
    "        + get embeddings from input sequence (using both embeddings\n",
    "          and positional embeddings if pos_enc is not None)\n",
    "        + run LSTM on embeddings\n",
    "        + use output of LSTM as an attention queries\n",
    "        + attend on the embedded sequence using queries (note autoregressive flag)\n",
    "        + make final linear tranformation to obtain logits\n",
    "        \"\"\"\n",
    "        # your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perm_len = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# time to set up a model\n",
    "# you can check that without pos_enc model doesn't work\n",
    "# not-autoregressive model can be learned easily, but it is less isefull\n",
    "# try to learn autoregressive model if possible\n",
    "pos_enc = PositionalEncoder(?, perm_len * 2, ?, ?)\n",
    "attention = ?\n",
    "model = PermMultiplier(perm_len, ?, ?, attention, pos_enc, ?)\n",
    "if torch.cuda.is_available():\n",
    "    model = model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up optimizer\n",
    "gd = optim.Adam(model.parameters(), lr=?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do optimization\n",
    "avg_loss = None\n",
    "forget = 0.99\n",
    "batch_size = 64\n",
    "iterator = range(?)\n",
    "for i in iterator:\n",
    "    gd.zero_grad()\n",
    "    batch = perm_generator(batch_size, perm_len)\n",
    "    if torch.cuda.is_available():\n",
    "        batch = batch[0].cuda(), batch[1].cuda()\n",
    "    # compute batch loss\n",
    "    # your code here\n",
    "    loss.backward()\n",
    "    if avg_loss is None:\n",
    "        avg_loss = float(loss)\n",
    "    else:\n",
    "        avg_loss = forget * avg_loss + (1 - forget) * float(loss)\n",
    "    descr_str = 'Iteration %05d, loss %.5f.' % (i, avg_loss)\n",
    "    print('\\r', descr_str, end='')\n",
    "    gd.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# time to check your model\n",
    "batch = perm_generator(batch_size, perm_len)\n",
    "if torch.cuda.is_available():\n",
    "    batch = batch[0].cuda(), batch[1].cuda()\n",
    "print('Input:\\n', batch[0][:5])\n",
    "print('Output:\\n', ?)\n",
    "print('Correct:\\n', batch[1][:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize attention map for some object\n",
    "# your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# play with model and learn something new about attention!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
