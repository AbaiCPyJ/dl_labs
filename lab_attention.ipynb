{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most recent version of this notebook is available at https://github.com/nadiinchi/dl_labs/blob/master/lab_attention.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook contains a practical introduction to the mechanisms of attention.\n",
    "The notebook is considering a toy problem and architecture, so it can be implemented and computed on the CPU.\n",
    "\n",
    "The task at hand is the multiplication (composition) of permutations.\n",
    "The length if permutations is fixed and equals perm_len.\n",
    "Input is an integer vector of length 2 x perm_len which contains two concatenated permutations p1 and p2.\n",
    "The product of p1 and p2 is a permutation p3 for which p3[i] = p1[p2[i]].\n",
    "The output of NN is also an integer vector of length 2 x perm_len in which first perm_lem elements are zero and the second perm_len elements are permutation p3.\n",
    "\n",
    "Example for perm_len = 5:\n",
    "```\n",
    "Input sequence:  3 4 2 1 0 1 3 0 2 4\n",
    "Output sequence: 0 0 0 0 0 4 1 3 2 0\n",
    "Clarification:  p1 = 3 4 2 1 0,    p2 = 1 3 0 2 4   =>    p3 = 4 1 3 2 0\n",
    "```\n",
    "\n",
    "Theoretically, such a problem can be solved by an ordinary LSTM, which will first memorize the permutation p1 in a hidden state, and then passing through the permutation p2 will produce the corresponding elements from the permutation p1.\n",
    "In practice, however, such a model works noticeably worse than a model with attention. A model with attention is explicitly learning by going through the p2 permutation to pay attention to the desired permutation element p1 and to output it.\n",
    "\n",
    "The task requires to implement and compare various types of attention used in real-life problems.\n",
    "You should also implement and use the position coding described in the article on Transformers.\n",
    "These layers and model are described in more detail below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "import numpy as np\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below it is proposed to implement several models of attention, described in different articles.\n",
    "In general, there are $K$ objects that you can pay attention to.\n",
    "Each object is characterized by the key $k_i$ and the value $v_i$.\n",
    "The attention layer proceeds requests.\n",
    "For the query $q$, the layer returns a weighted sum of the values of the objects, with weights proportional to the degree of key matching the query:\n",
    "$$w_i = \\frac{\\exp(score(q, k_i))}{\\sum_{j=1}^K\\exp(score(q, k_j))}$$\n",
    "$$a = \\sum_{i=1}^K w_i v_i$$\n",
    "\n",
    "Almost always, queries, keys, and values are real vectors of some fixed dimensions.\n",
    "In the assignment it is proposed to implement three types of attention:\n",
    "+ (optional!) Additive Attention.\n",
    "Defined by function $ score (q, k) = w_3 ^ T \\ tanh (W_1q + W_2k) $, where $ W_1, W_2, w_3 $ are the trainable parameters of the attention layer.\n",
    "For such a function, the request and key may have different dimensions.\n",
    "Matrices $ W_1 $ and $ W_2 $ map the query and key into a common hidden space, the dimension of which coincides with the dimension of the vector $ w_3 $.\n",
    "The dimension of the hidden space can be chosen arbitrarily and is a hyperparameter of the layer.\n",
    "For more information see the paper Bahdanau et al. \"Neural Machine Translation by Jointly Learning to Align and Translate\", 2014.\n",
    "+ Multiplicative Attention.\n",
    "Defined by function $score(q, k) = q^Tk$.\n",
    "To use this type of attention, it is required that the dimension of the query coincides with the dimension of the key.\n",
    "For more information see the paper Luong et al. \"Effective approaches to attention-based neural machine translation\", 2015.\n",
    "+ Scaled Dot Product Attention.\n",
    "Defined by function $score(q, k) = \\frac{q^Tk}{\\sqrt{dim(k)}}$, where $dim(k)$ is the dimensionality of the key (which also equals the dimensionality of the query).\n",
    "With learnable queries or keys, such attention is equivalent to multiplicative attention described above.\n",
    "However, immediately after initialization, such attention encourages smoother weights, which alleviates the problem of small gradients for saturated SoftMax.\n",
    "For more information see the paper Vaswani et al. \"Attention Is All You Need\", 2017.\n",
    "\n",
    "In practice, the architecture in which the keys and values of the objects are the same vector is used.\n",
    "In the prototypes below, it is proposed to implement such an architecture.\n",
    "Since keys and values are the same, they are passed to the function only once and are called objects' features\n",
    "(i. e. $f_i := k_i = v_i$).\n",
    "\n",
    "Also, for the flexibility of the interface and for the acceleration of learning, all layers of attention below receive several requests for each object of the batch.\n",
    "\n",
    "Attention class is the parent of AdditiveAttention, MultiplicativeAttention, and ScaledDotProductAttention classes.\n",
    "Attention class is an abstract class, so it is never used as layer, and only its subclasses are used as layers.\n",
    "\n",
    "In Attention class it is necessary to implement function attend, which accepts a set of feature attributes and a set of requests for each batch object.\n",
    "Function attend uses function get_scores, which is implemented in all class subclasses, and then use the obtained $score(q, f)$ values to compute $w$ and $a$.\n",
    "\n",
    "Mask is an attention mask, showing for each request which objects it cannot pay attention to.\n",
    "It was proposed in the paper Vaswani et al. \"Attention Is All You Need\", 2017.\n",
    "It is used to ensure that the learning model retains its autoregressive properties, that is, that the layer output for the $i$ -th position does not depend on the input values of subsequent positions.\n",
    "In function get_autoregressive_mask, it is necessary to construct the above-described square mask of a given size.\n",
    "\n",
    "The most numerically stable way to use a mask in attend will be to set the corresponding score values to -float('inf') before applying SoftMax.\n",
    "An alternative method is to zero the scales $w$ according to the mask and renormalize them, but this method is less computationally stable (think why).\n",
    "\n",
    "In each of classes AdditiveAttention, MultiplicativeAttention, and ScaledDotProductAttention, you need to implement function get_scores, which for each batch object for each request returns its similarity to objects of the same batch.\n",
    "The get_scores code should be equivalent to the following:\n",
    "```\n",
    "res = torch.zeros(batch_size, num_queries, num_objects)\n",
    "for i in range(batch_size):\n",
    "    for j in range(num_queries):\n",
    "        for k in range(num_objects):\n",
    "            res[i, j, k] = score(queries[i, j], features[i, k])\n",
    "```\n",
    "Naturally, the above code is only an illustration explaining the dimensions of the arguments and the output of the get_scores function.\n",
    "Your implementation must be effectively vectorized.\n",
    "\n",
    "Implementation hints:\n",
    "\n",
    "+ In class AdditiveAttention, you need to have the learnable parameters $W_1$, $W_2$, and $w_3$.\n",
    "  * For those who want to practice writing their own learnable layers, remember that the tensor contained in the subclass of nn.Module will not be listed in .parameters() for an object of this class, so gradient descent will not affect this tensor.\n",
    "In order for the tensor to appear in .parameters(), you need to wrap it in nn.Parameter().\n",
    "Also, before such a wrapper, it should be initialized using one of the standard initializations of the pytorch layers.\n",
    "It should be noted that the initializations in pytorch are in-place, that is, you first need to have a tensor, and then pass it to the initialization function.\n",
    "  * For those who do not wish to practice writing their own learnable layers, we can recall that multiplying by a matrix is equivalent to using nn.Linear(.., bias=False).\n",
    "Thus, the attention function can be implemented easily using three linear layers corresponding to matrices $W_1$, $W_2$, Ð¸ $w_3$.\n",
    "+ It is recommended to pay attention to the function torch.bmm, it can be useful in many layers of attention below.\n",
    "+ To visualize the attention map at the end of the notebook, you need to save weights.detach() in self.last_weights in the attend function. .detach () is used so that the stored weights are not part of the computational graph. Do not forget to do .detach() for the debug output, so that the computational graph does not consume RAM beyond the required size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_autoregressive_mask(size):\n",
    "    \"\"\"\n",
    "    Returns attention mask of given size for autoregressive model.\n",
    "    \"\"\"\n",
    "    # your code here\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def get_scores(self, features, queries):\n",
    "        \"\"\"\n",
    "        features: [batch_size x num_objects x obj_feature_dim]\n",
    "        queries:  [batch_size x num_queries x query_feature_dim]\n",
    "        Returns matrix of scores with shape [batch_size x num_queries x num_objects].\n",
    "        \"\"\"\n",
    "        raise NotImplementedError()                \n",
    "\n",
    "    def attend(self, features, queries, mask=None):\n",
    "        \"\"\"\n",
    "        features:        [batch_size x num_objects x obj_feature_dim]\n",
    "        queries:         [batch_size x num_queries x query_feature_dim]\n",
    "        mask, optional:  [num_queries x num_objects]\n",
    "        Returns matrix of features for queries with shape [batch_size x num_queries x obj_feature_dim].\n",
    "        If mask is not None, sets corresponding to mask weights to zero.\n",
    "        Saves detached weights as self.last_weights for further visualization.\n",
    "        \"\"\"\n",
    "        # your code here\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdditiveAttention(Attention):\n",
    "    \"\"\"\n",
    "    Bahdanau et al. \"Neural Machine Translation by Jointly Learning to Align and Translate\", 2014.\n",
    "    \"\"\"\n",
    "    def __init__(self, obj_feature_dim, query_feature_dim, hidden_dim):\n",
    "        \"\"\"\n",
    "        obj_feature_dim   - dimensionality of attention object features vector\n",
    "        query_feature_dim - dimensionality of attention query vector\n",
    "        hidden_dim        - dimensionality of latent vectors of attention \n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        # your code here\n",
    "\n",
    "    def get_scores(self, features, queries):\n",
    "        \"\"\"\n",
    "        features: [batch_size x num_objects x obj_feature_dim]\n",
    "        queries:  [batch_size x num_queries x query_feature_dim]\n",
    "        Returns matrix of scores with shape [batch_size x num_queries x num_objects].\n",
    "        \"\"\"\n",
    "        # your code here\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiplicativeAttention(Attention):\n",
    "    \"\"\"\n",
    "    Luong et al. \"Effective approaches to attention-based neural machine translation\", 2015.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def get_scores(self, features, queries):\n",
    "        \"\"\"\n",
    "        features: [batch_size x num_objects x feature_dim]\n",
    "        queries:  [batch_size x num_queries x feature_dim]\n",
    "        Returns matrix of scores with shape [batch_size x num_queries x num_objects].\n",
    "        \"\"\"\n",
    "        # your code here\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScaledDotProductAttention(Attention):\n",
    "    \"\"\"\n",
    "    Vaswani et al. \"Attention Is All You Need\", 2017.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def get_scores(self, features, queries):\n",
    "        \"\"\"\n",
    "        features: [batch_size x num_objects x feature_dim]\n",
    "        queries:  [batch_size x num_queries x feature_dim]\n",
    "        Returns matrix of scores with shape [batch_size x num_queries x num_objects].\n",
    "        \"\"\"\n",
    "        # your code here\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# time to check that your attention works\n",
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The perm_generator function generates a batch of a given size of objects for training or a test.\n",
    "For each object in the batch permuations p1 and p2 of length perm_size are generated equiprobable.\n",
    "They form the input sequence [p1, p2] and the correct answer [0, p3] for it (see the example above)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perm_generator(batch_size, perm_size):\n",
    "    \"\"\"\n",
    "    Generates batch of batch_size objects.\n",
    "    Each object consists of two random permutations with length perm_size.\n",
    "    The target for the object is the product of its two permutations.\n",
    "    \"\"\"\n",
    "    # your code here\n",
    "    return objects, correct_answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# time to check your generator\n",
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PositionalEncoder is the layer described in Vaswani et al. \"Attention Is All You Need\", 2017.\n",
    "Adds to the output of the previous layer embedding positions.\n",
    "In order not to recalculate position embeddings each time, its constructor receives the max_len parameter and precomputes embeddingings for positions from 0 to max_len - 1 inclusive.\n",
    "The add flag indicates whether to add position embeddings to the output of the previous layer (by default, with add = True, as was in the original paper) or concatenated (add = False).\n",
    "For the selected embedding dimensions, you should visualize the embeddings (plot the each component of the embeddings) and select the appropriate scale parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoder(nn.Module):\n",
    "    def __init__(self, dim, max_len=50, scale=10000.0, add=True):\n",
    "        \"\"\"\n",
    "        Transforms input as described by Vaswani et al. in \"Attention Is All You Need\", 2017.\n",
    "        dim     - dimension of positional embeddings.\n",
    "        max_len - maximal length of sequence, for precomputing\n",
    "        scale   - scale factor for frequency for positional embeddings\n",
    "        add     - boolean, if add is False, concatenate positional embeddings with input instead of adding\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        \n",
    "        self.dim = dim\n",
    "        self.add = add\n",
    "        if add:\n",
    "            self.extra_output_shape = 0\n",
    "        else:\n",
    "            self.extra_output_shape = dim\n",
    "\n",
    "        # your code here\n",
    "               \n",
    "    def forward(self, input):\n",
    "        \"\"\"\n",
    "        input - [batch_size x sequence_len x features_dim]\n",
    "        If self.add is True, self.dim = featurs_dim.\n",
    "        Returns input with added or concatenated positional embeddings (depending on self.add).\n",
    "        \"\"\"\n",
    "        # your code here\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# time to draw positional encoder\n",
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model consists of the following layers:\n",
    "+ Embedding elements of the input sequence.\n",
    "+ Positional embedding or None, meaning the absence of this layer.\n",
    "+ LSTM network.\n",
    "To compute the LSTM network input dimension, you can use the dimension of the first embedding and extra_output_shape for embedding of the positions.\n",
    "+ A layer of attention. It receives the outputs of the lSTM network as requests  and sequence embeddings as objects.\n",
    "When autoregressive flag is True, this layer uses the attention mask to ignore sequence elements from the future.\n",
    "+ Logistic regression with perm_len classes, outputs an integer for each position of answer.\n",
    "\n",
    "Please note that this model is neither a transformer nor a traditional network using LSTM with attention.\n",
    "The transformer uses K-head attention, elementwise transformation of embeddings.\n",
    "In traditional networks with LSTM, the request issued by the network at the previous time point affects the network input at the next time point, therefore simultaneous parallel processing of the entire sequence is impossible.\n",
    "Also, most networks use the encoder-decoder architecture, where the encoder first reads the entire input sequence and forms its hidden representation, and then the decoder outputs the output sequence using this hidden representation and attention mechanism."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PermMultiplier(nn.Module):\n",
    "    def __init__(self, perm_len, embedding_dim, hidden_dim, attention, pos_enc, autoregressive):\n",
    "        \"\"\"\n",
    "        perm_len       - permutation length (the input is twice longer)\n",
    "        embedding_dim  - dimensionality of integer embeddings\n",
    "        hidden_dim     - dimensionality of LSTM output\n",
    "        attention      - Attention object\n",
    "        pos_enc        - PositionalEncoder object or None\n",
    "        autoregressive - boolean, if True, then model must use autoregressive mask for attention\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.autoregressive = autoregressive\n",
    "        self.perm_len = perm_len\n",
    "        # your code here\n",
    "\n",
    "    def forward(self, input):\n",
    "        \"\"\"\n",
    "        Perform forward pass through layers:\n",
    "        + get embeddings from input sequence (using both embeddings\n",
    "          and positional embeddings if pos_enc is not None)\n",
    "        + run LSTM on embeddings\n",
    "        + use output of LSTM as an attention queries\n",
    "        + attend on the embedded sequence using queries (note autoregressive flag)\n",
    "        + make final linear tranformation to obtain logits\n",
    "        \"\"\"\n",
    "        # your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a time to write models and a time to train them.\n",
    "Now came the second one.\n",
    "\n",
    "Complete the code for learning the model below.\n",
    "Find the right architecture and set of hyperparameters for the model and optimization method.\n",
    "For some architecture and hyperparameters the model can be trained on the CPU in a short time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perm_len = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# time to set up a model\n",
    "# you can check that without pos_enc model doesn't work\n",
    "# not-autoregressive model can be learned easily, but it is less isefull\n",
    "# try to learn autoregressive model if possible\n",
    "pos_enc = PositionalEncoder(?, perm_len * 2, ?, ?)\n",
    "attention = ?\n",
    "model = PermMultiplier(perm_len, ?, ?, attention, pos_enc, ?)\n",
    "if torch.cuda.is_available():\n",
    "    model = model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up optimizer\n",
    "gd = optim.Adam(model.parameters(), lr=?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do optimization\n",
    "avg_loss = None\n",
    "forget = 0.99\n",
    "batch_size = 64\n",
    "iterator = range(?)\n",
    "for i in iterator:\n",
    "    gd.zero_grad()\n",
    "    batch = perm_generator(batch_size, perm_len)\n",
    "    if torch.cuda.is_available():\n",
    "        batch = batch[0].cuda(), batch[1].cuda()\n",
    "    # compute batch loss\n",
    "    # your code here\n",
    "    loss.backward()\n",
    "    if avg_loss is None:\n",
    "        avg_loss = float(loss)\n",
    "    else:\n",
    "        avg_loss = forget * avg_loss + (1 - forget) * float(loss)\n",
    "    descr_str = 'Iteration %05d, loss %.5f.' % (i, avg_loss)\n",
    "    print('\\r', descr_str, end='')\n",
    "    gd.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great, we have some model.\n",
    "Let's check how it multiplies two random permutations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# time to check your model\n",
    "batch = perm_generator(batch_size, perm_len)\n",
    "if torch.cuda.is_available():\n",
    "    batch = batch[0].cuda(), batch[1].cuda()\n",
    "print('Input:\\n', batch[0][:5])\n",
    "print('Output:\\n', ?)\n",
    "print('Correct:\\n', batch[1][:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the important for applications properties of attention is to show what the model pays attention to.\n",
    "For this, so-called attention maps are used.\n",
    "Use the last_weights field of the Attention layer to visualize which positions the trained model paid attention to at each point in time for permutations from the batch in the cell above.\n",
    "Expected behavior is that for each element of permutation p2 model pays attention to the corresponding element from the permutation p1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize attention map for some object\n",
    "# your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# play with model and learn something new about attention!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
