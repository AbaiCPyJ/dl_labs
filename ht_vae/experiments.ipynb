{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Вариационный автокодировщик"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В задании предлагается реализовать вариационный и обычный автокодировщики, обучить их на MNIST, сравнить между собой эти модели, сделать выводы по результатам сравнения и выводы про каждую модель по отдельности.\n",
    "\n",
    "Необходимая теория приведена ниже. Для более глубокого погружения в тему также есть список литературы с комментариями.\n",
    "\n",
    "В этом задании нельзя использовать функций плотностей распределений, KL-дивергенцию и репараметризации из стандартных библиотек."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Актуальная версия доступна по адресу https://github.com/nadiinchi/dl_labs/blob/master/ht_vae/experiments.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Теория"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Постановка задачи\n",
    "Дана выборка независимых одинаково распределенных величин из истинного распределения $x_i \\sim p_d(x)$, $i = 1, \\dots, N$.\n",
    "\n",
    "Задача - построить вероятностную модель $p_\\theta(x)$ истинного распределения $p_d(x)$.\n",
    "\n",
    "Распределение $p_\\theta(x)$ должно позволять как оценить плотность вероятности для данного объекта $x$, так и сэмплировать $x \\sim p_\\theta(x)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Вероятностная модель\n",
    "$z \\in \\mathbb{R}^d$ - локальная латентная переменная, т. е. своя для каждого объекта $x$.\n",
    "\n",
    "Генеративный процесс вариационного автокодировщика:\n",
    "1. Сэмплируем $z \\sim p(z)$.\n",
    "2. Сэмплируем $x \\sim p_\\theta(x | z)$.\n",
    "\n",
    "Параметры распределения $p_\\theta(x | z)$ задаются нейросетью с весами $\\theta$, получающей на вход вектор $z$. Эта сеть называется генеративной сетью (generator) или декодером (decoder).\n",
    "\n",
    "Индуцированная генеративным процессом плотность вероятности объекта $x$:\n",
    "\n",
    "$$p_\\theta(x) = \\mathbb{E}_{z \\sim p(z)} p_\\theta(x | z)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Параметризация модели\n",
    "Априорное распределение на скрытые перменные - стандартное нормальное распределение: $p(z) = \\mathcal{N}(z | 0, I)$.\n",
    "\n",
    "Распределения на компоненты $x$ условно независимы относительно $z$: $p_\\theta(x | z) = \\prod\\limits_{i = 1}^D p_\\theta(x_i | z)$.\n",
    "\n",
    "Если i-ый признак объекта вещественный, то $p_\\theta(x_i | z) = \\mathcal{N}(x_i | \\mu_i(z, \\theta), \\sigma^2_i(z, \\theta))$.\n",
    "Здесь $\\mu(z, \\theta)$ и $\\sigma(z, \\theta)$ - детерминированные функции, задаваемые нейросетями с параметрами $\\theta$.\n",
    "\n",
    "Если i-ый признак категориальный, то $p_\\theta(x_i | z) = Cat(Softmax(\\omega_i(z, \\theta)))$, где $\\omega_i(z, \\theta)$ - тоже детерминированная функция задаваемая нейросетью.\n",
    "\n",
    "Отдельно можно рассмотреть бинарные признаки, для которых категориальное распределение превращается в распределение Бернулли с одним параметром."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Вариационная нижняя оценка логарифма правдоподобия\n",
    "\n",
    "Для максимизации правдоподобия максимизируем вариационную нижнюю оценку на логарифм правдоподобия:\n",
    "$$\\log p_\\theta(x) = \\mathbb{E}_{z \\sim q_\\phi(z | x)} \\log p_\\theta(x) = \n",
    "\\mathbb{E}_{z \\sim q_\\phi(z | x)} \\log \\frac{p_\\theta(x, z) q_\\phi(z | x)}{q_\\phi(z | x) p_\\theta(z | x)} = \n",
    "\\mathbb{E}_{z \\sim q_\\phi(z | x)} \\log \\frac{p_\\theta(x, z)}{q_\\phi(z | x)} + KL(q_\\phi(z | x) || p_\\theta(z | x))$$\n",
    "$$\\log p_\\theta(x) \\geqslant \\mathbb{E}_{z \\sim q_\\phi(z | x)} \\log \\frac{p_\\theta(x | z)p(z)}{q_\\phi(z | x)} = \n",
    "\\mathbb{E}_{z \\sim q_\\phi(z | x)} \\log p_\\theta(x | z) - KL(q_\\phi(z | x) || p(z)) = L(x; \\phi, \\theta)\n",
    "\\to \\max\\limits_{\\phi, \\theta}$$\n",
    "\n",
    "$q_\\phi(z | x)$ называется предложным (proposal), распознающим (recognition) или вариационным (variational) распределением. Это гауссиана, чьи параметры задаются нейросетью с весами $\\phi$:\n",
    "$q_\\phi(z | x) = \\mathcal{N}(z | \\mu_\\phi(x), \\sigma^2_\\phi(x)I)$.\n",
    "Обычно нейросеть моделирует не $\\sigma_\\phi(x)$, а $\\log\\sigma_\\phi(x)$ или $\\log(\\exp(\\sigma_\\phi(x) - 1))$ или другую величину, более инвариантную к масштабу и определенную на всех вещественных числах так, чтобы $\\sigma_\\phi(x)$ было всегда положительным. В этом задании нужно использовать $\\log(\\exp(\\sigma_\\phi(x) - 1))$, поскольку обратное к нему преобразование softplus более стабильно, чем экспонента.\n",
    "\n",
    "Зазор между вариационной нижней оценкой $L(x; \\phi, \\theta)$ на логарифм правдоподобия модели и самим логарифмом правдоподобия $\\log p_\\theta(x)$ - это KL-дивергенция между предолжным и апостериорным распределением на $z$: $KL(q_\\phi(z | x) || p_\\theta(z | x))$. Максимальное значение $L(x; \\phi, \\theta)$ при фиксированных параметрах модели $\\theta$ достигается при $q_\\phi(z | x) = p_\\theta(z | x)$, но явное вычисление $p_\\theta(z | x)$ требует слишком большого числа ресурсов, поэтому вместо этого вычисления вариационная нижняя оценка оптимизируется также по $\\phi$. Чем ближе $q_\\phi(z | x)$ к $p_\\theta(z | x)$, тем точнее вариационная нижняя оценка.\n",
    "Истинное апостериорное распределение $p_\\theta(z | x)$ часто не может быть представлено одной гауссианой, поэтому зазор между нижней оценкой и логарифмом правдоподобия не достигает $0$. Тем не менее, есть статьи, утверждающие, что этот зазор практически не влияет на процесс оптимизации модели и его результат по сравнению с другими факторами (см. литературу).\n",
    "\n",
    "Первое слагаемое вариационной нижней оценки $\\mathbb{E}_{z \\sim q_\\phi(z | x)} \\log p_\\theta(x | z)$ называется ошибкой восстановления (reconstruction loss).\n",
    "Модель, соответствующая этой части - это автокодирощик с одним стохастическим слоем, пытающийся восстановить входной объект $x$.\n",
    "Если распределение $q_\\phi(z | x)$ - дельта-функция, то автокодировщик со стохастическим слоем превращается в обычный автокодировщик.\n",
    "Поэтому $q_\\phi(z | x)$ и $p_\\theta(x | z)$ иногда называют энкодером и декодером соответственно.\n",
    "\n",
    "Слагаемое $KL(q_\\phi(z | x) || p(z))$ иногда называют регуляризатором.\n",
    "Оно вынуждает $z \\sim q_\\phi(z | x)$ быть близким к $0$ и $q_\\phi(z | x)$ быть близким к $p_\\theta(z | x)$.\n",
    "Иногда коэффициент при KL-дивергенции полагают не равным единице или даже используют другой регуляризатор.\n",
    "Естественно, после этого обучение модели перестает соответствовать максимизации правдоподобия вышеописанной вероятностной модели данных.\n",
    "Это существенно снижает интерпретируемость модели, устраняет теоретические гарантии для неё.\n",
    "KL-дивергенция между двумя нормальными распределениями может быть вычислена аналитически.\n",
    "\n",
    "Для максимизации $L(x; \\phi, \\theta)$ используется стохастический градиентный подъем.\n",
    "Градиент ошибки восстановления по $\\theta$ вычисляется с помощью метода обратного распространения ошибки.\n",
    "$$\\frac{\\partial}{\\partial \\theta} L(x; \\phi, \\theta) = \\mathbb{E}_{z \\sim q_\\phi(z | x)} \\frac{\\partial}{\\partial \\theta} \\log p_\\theta(x | z)$$\n",
    "\n",
    "Градиент KL-дивергенции по $\\phi$ может быть вычислен аналитически.\n",
    "Для вычисления градиента ошибки восстановления по $\\phi$ используется репараметризация (reparametrization trick):\n",
    "$$\\varepsilon \\sim \\mathcal{N}(\\varepsilon | 0, I)$$\n",
    "$$z = \\mu + \\sigma \\varepsilon \\Rightarrow z \\sim \\mathcal{N}(z | \\mu, \\sigma^2I)$$\n",
    "$$\\frac{\\partial}{\\partial \\phi} L(x; \\phi, \\theta) = \\mathbb{E}_{\\varepsilon \\sim \\mathcal{N}(\\varepsilon | 0, I)} \\frac{\\partial}{\\partial \\phi} \\log p_\\theta(x | \\mu_\\phi(x) + \\sigma_\\phi(x) \\varepsilon) - \\frac{\\partial}{\\partial \\phi} KL(q_\\phi(z | x) || p(z))$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Подсказка: для выведения аналиической формулы KL-дивергенции между нормальными распределениями главное - никогда не писать знак интеграла. В задании рассматриваются только нормальные распределения с диагональной матрицей ковариации, поэтому достаточно вывести KL-дивергенцию между двумя одномерными нормальными распределениями. Все, что требуется для выведения формулы:\n",
    "$$KL(q || p) = \\mathbb{E}_{z \\sim q} \\log\\frac{q(z)}{p(z)}$$\n",
    "$$\\log \\mathrm{N}(z | \\mu, \\sigma^2) = \\frac{1}{\\sqrt{2\\pi}\\sigma} \\exp \\left(-\\frac{(z - \\mu)^2}{2\\sigma^2} \\right)$$\n",
    "$$\\mathbb{E}_{z \\sim N(\\mu, \\sigma)}z = \\mu$$\n",
    "$$\\mathbb{E}_{z \\sim N(\\mu, \\sigma)}z^2 = \\mu^2 + \\sigma ^ 2$$\n",
    "Кстати, у трех последних формул есть часто используемые многомерные обощения."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Оценка правдоподобия модели\n",
    "\n",
    "Правдоподобие модели $p_\\theta(x) = \\mathbb{E}_{z \\sim p(z)} p_\\theta(x | z)$ оценивают на отложенной выборке.\n",
    "\n",
    "Оценка может быть получена с помощью метода Монте-Карло:\n",
    "\n",
    "$$z_i \\sim p(z), i = 1, \\dots, K$$\n",
    "$$p_\\theta(x) \\approx \\frac{1}{K} \\sum\\limits_{i = 1}^K p_\\theta(x | z_i)$$\n",
    "\n",
    "Альтернативный способ оценки - метод importance sampling. В качестве предложного распределения метода используется предложное распределение модели. Известно, что хороший выбор предложного распределения уменьшает дисперсию оценки. Для вариационных автокодировщиков оценки Монте-Карло, основанные на малом числе сэмплов, обычно занижены. Поэтому imporatance sampling также позволяет получить более высокую и точную оценку правдоподобия с помощью меньшего числа сэмплов.\n",
    "\n",
    "$$z_i \\sim q_\\phi(z | x), i = 1, \\dots, K$$\n",
    "$$p_\\theta(x) = \\mathbb{E}_{z \\sim p(z)} p_\\theta(x | z) = \\mathbb{E}_{z \\sim q_\\phi(z | x)} \\frac{p_\\theta(x | z) p(z)}{q_\\phi(z | x)} \\approx \\frac{1}{K} \\sum\\limits_{i = 1}^K \\frac{p_\\theta(x | z_i) p(z_i)}{q_\\phi(z_i | x)}$$\n",
    "\n",
    "Для оценки логарифма правдоподобия усреднение вероятностей происходит под логарифмом:\n",
    "$$\\log p_\\theta(x) \\approx \\log \\frac{1}{K} \\sum\\limits_{i = 1}^K p_\\theta(x | z_i),\\,\\,\\,\\,z_i \\sim p(z)$$\n",
    "$$\\log p_\\theta(x) \\approx \\log \\frac{1}{K} \\sum\\limits_{i = 1}^K \\frac{p_\\theta(x | z_i) p(z_i)}{q_\\phi(z_i | x)},\\,\\,\\,\\,z_i \\sim q_\\phi(z | x)$$\n",
    "Заметим, что оценки логарифма правдоподобия уже не являются несмещенными, в отличие от оценок самого правдоподобия.\n",
    "Несмотря на это, первую оценку все равно обычно называют оценкой Монте-Карло логарифма правдоподобия.\n",
    "Вторая оценка известна в литературе как IWAE оценка по названию модели Importance Weighted Variational Autoencoders, в которой предлагается напрямую оптимизировать эту оценку правдоподобия для обучения автокодировщика."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Литература\n",
    "1. Auto-Encoding Variational Bayes https://arxiv.org/pdf/1312.6114.pdf, Stochastic Backpropagation and Approximate Inference in Deep Generative Models https://arxiv.org/pdf/1401.4082.pdf - оригинальные статьи про вариационный автокодировщик (две группы авторов независимо и почти одновременно предложили одинаковые модели).\n",
    "2. Learning Structured Output Representation using Deep Conditional Generative Models https://papers.nips.cc/paper/5775-learning-structured-output-representation-using-deep-conditional-generative-models.pdf - обусловленный вариационный автокодировщик для генерации из обусловленного распределения на объекты.\n",
    "3. Importance Weighted Autoencoders https://arxiv.org/pdf/1509.00519.pdf - вариационный автокодировщик, оптимизирующий более точную нижнюю оценку на логарифм правдоподобия.\n",
    "4. Tighter Variational Bounds are Not Necessarily Better https://arxiv.org/pdf/1802.04537.pdf - статья, показывающая, что предыдущая статья ухудшает обучение предложной сети автокодировщика как следствие более хорошей нижней оценки, и предлагающая способы решения этой проблемы.\n",
    "5. Variational Inference with Normalizing Flows https://arxiv.org/pdf/1505.05770.pdf, Improved Variational Inference with Inverse Autoregressive Flow http://papers.nips.cc/paper/6581-improved-variational-inference-with-inverse-autoregressive-flow.pdf - более богатые семейства предложных распредлений.\n",
    "6. VAE with a VampPrior https://arxiv.org/pdf/1705.07120.pdf - обучение априорного распределения на скрытые представления совместно с предложным. Улучшает правдоподобие модели, но делает менее интерпретируемым скрытое пространство.\n",
    "7. Ladder Variation Autoencoders http://papers.nips.cc/paper/6275-ladder-variational-autoencoders.pdf - теперь у каждого объекта есть не одно скрытое представление, а несколько организованных в иерархию.\n",
    "8. Inference Suboptimality in Variational Autoencoders https://arxiv.org/pdf/1801.03558.pdf - утверждает, что отличие предложного распределения от истинного апостериорного распределения в скрытом пространстве вызваны недостаточной выразительностью предложной сети, а не бедным семейством предложных распределений. Богатое же семейство предложных моделей снижает требования к выразительности предложной сети."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Практика"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Загрузка, предобработка и визуалиация данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.datasets import MNIST\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = MNIST('mnist', download=True, train=True)\n",
    "train_data = TensorDataset(data.train_data.view(-1, 28 * 28).float() / 255, data.train_labels)\n",
    "data = MNIST('mnist', download=True, train=False)\n",
    "test_data = TensorDataset(data.test_data.view(-1, 28 * 28).float() / 255, data.test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "def show_images(x):\n",
    "    plt.figure(figsize=(12, 12 / 10 * (x.shape[0] // 10 + 1)))\n",
    "    x = x.view(-1, 28, 28)\n",
    "    for i in range(x.shape[0]):\n",
    "        plt.subplot(x.shape[0] // 10 + 1, 10, i + 1)\n",
    "        plt.imshow(x.data[i].numpy(), cmap='Greys_r', vmin=0, vmax=1, interpolation='lanczos')\n",
    "        plt.axis('off')\n",
    "\n",
    "show_images(train_data[:10][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для корректности вероятностной модели исходное изображение также должно быть бинаризовано.\n",
    "\n",
    "Бинаризация может производиться как округлением данных в датасете, так и сэмплированием из распределения Бернулли каждого пикселя. Округление приводит к более гладким фигурам в обучающей выборке, поэтому будем использовать его."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_images(torch.bernoulli(train_data[:10][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_images(train_data[:10][0].round())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.tensors = (train_data.tensors[0].round(), train_data.tensors[1])\n",
    "test_data.tensors = (test_data.tensors[0].round(), test_data.tensors[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Вспомогательные функции для обучения и тестирования"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_test_loss(compute_loss, batch_size=100, max_batches=None, verbose=False):\n",
    "    \"\"\"\n",
    "    Функция вычисляет усредненное значение функции потерь по тестовым данным.\n",
    "    Вход: compute_loss, функция, принимающая батч в виде матрицы torch.FloatTensor\n",
    "    и возвращающая float - функцию потерь на батче.\n",
    "    Вход: batch_size, int.\n",
    "    Вход: max_batches, int - если задано, включает режим оценки функции потерь\n",
    "    с помощью сэмплирования батчей вместо полного прохода по данным и указывает,\n",
    "    после какого батча прекратить вычисления.\n",
    "    Вход: verbose, bool - указывает, печатать ли текущее состояние в процессе работы.\n",
    "    Возвращаемое значение: float - оценка функции потерь на тестовых данных.\n",
    "    \"\"\"\n",
    "    dataloader = DataLoader(test_data, batch_size=batch_size, shuffle=(max_batches is None))\n",
    "    num_batches = len(dataloader)\n",
    "    avg_loss = 0\n",
    "    for i, (batch, _) in enumerate(dataloader):\n",
    "        loss = compute_loss(batch)\n",
    "        avg_loss += (loss - avg_loss) / (i + 1)\n",
    "        if verbose and (i + 1) % 10 == 0:\n",
    "            print('\\rTest loss:', avg_loss,\n",
    "                  'Batch', i + 1, 'of', num_batches, ' ' * 10, end='', flush=True)\n",
    "        if verbose and (i + 1) % 100 == 0:\n",
    "            print(flush=True)\n",
    "        if max_batches and i >= max_batches:\n",
    "            break\n",
    "    return avg_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, tests=[], batch_size=100, num_epochs=5, learning_rate=1e-3, maximization=True):\n",
    "    \"\"\"\n",
    "    Обучает модель.\n",
    "    Вход: model, Module - объект, модель.\n",
    "    У этого объекта должна быть функция batch_loss от batch - FloatTensor и K - int,\n",
    "    возвращающая скаляр Variable - функцию потерь на батче, которая должна быть\n",
    "    оптимизирована.\n",
    "    Вход: tests - список тестов, выполняемых после каждого 100-го батча.\n",
    "    Каждый элемент списка - словарь с полями 'name' - уникальным идентификатором\n",
    "    теста и 'func' - функцией от модели.\n",
    "    Вход: batch_size, int.\n",
    "    Вход: num_epochs, int.\n",
    "    Вход: learning_rate, float.\n",
    "    Возвращаемое значение: словарь с полями 'model' - обученная модель,\n",
    "    'train_losses_list' - список функций потерь на каждом батче и \n",
    "    'test_results' - список результатов тестирования. Каждый результат\n",
    "    тестирования - словарь вида name: value, где name - имя теста,\n",
    "    value - результат его выполнения.\n",
    "    \"\"\"\n",
    "    gd = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    dataloader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "    train_losses = []\n",
    "    test_results = []\n",
    "    for _ in range(num_epochs):\n",
    "        for i, (batch, _) in enumerate(dataloader):\n",
    "            total = len(dataloader)\n",
    "            loss = model.batch_loss(batch)\n",
    "            if maximization:\n",
    "                (-loss).backward()\n",
    "            else:\n",
    "                loss.backward()\n",
    "            train_losses.append(float(loss))\n",
    "            if (i + 1) % 10 == 0:\n",
    "                print('\\rTrain loss:', train_losses[-1],\n",
    "                      'Batch', i + 1, 'of', total, ' ' * 10, end='', flush=True)\n",
    "            if (i + 1) % 100 == 0:\n",
    "                cur_test_result = {}\n",
    "                for test in tests:\n",
    "                    cur_test_result[test['name']] = test['func'](model)\n",
    "                test_results.append(cur_test_result)\n",
    "                print(flush=True)\n",
    "            gd.step()\n",
    "            gd.zero_grad()\n",
    "    return {\n",
    "        'model': model,\n",
    "        'train_losses_list': train_losses,\n",
    "        'test_results': test_results\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 15\n",
    "digit_size = 28\n",
    "\n",
    "from scipy.stats import norm\n",
    "import numpy as np\n",
    "\n",
    "grid_x = norm.ppf(np.linspace(0.05, 0.95, n))\n",
    "grid_y = norm.ppf(np.linspace(0.05, 0.95, n))\n",
    "\n",
    "def draw_manifold(generator):\n",
    "    figure = np.zeros((digit_size * n, digit_size * n))\n",
    "    for i, yi in enumerate(grid_x):\n",
    "        for j, xi in enumerate(grid_y):\n",
    "            z_sample = np.array([[xi, yi]])\n",
    "\n",
    "            x_decoded = generator(z_sample)\n",
    "            digit = x_decoded\n",
    "            figure[i * digit_size: (i + 1) * digit_size,\n",
    "                   j * digit_size: (j + 1) * digit_size] = digit\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    plt.imshow(figure, cmap='Greys_r', vmin=0, vmax=1, interpolation='lanczos')\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_latent_space(data, target, encoder):\n",
    "    z_test = encoder(data)\n",
    "    plt.figure(figsize=(7, 6))\n",
    "    plt.scatter(z_test[:, 0], z_test[:, 1], c=target, cmap='gist_rainbow', alpha=0.75)\n",
    "    plt.colorbar()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Обычный автокодировщик"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models import AE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Обучение моделей"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ae_tests = [\n",
    "    {\n",
    "        'name': 'test_loss',\n",
    "        'func': lambda model:\n",
    "                model_test_loss(lambda batch:\n",
    "                                float(model.batch_loss(batch)),\n",
    "                                max_batches=20)\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ae_model_d2 = train_model(AE(2, 784), tests=ae_tests, maximization=False, num_epochs=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ae_model_d10 = train_model(AE(10, 784), tests=ae_tests, maximization=False, num_epochs=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Оценка качества моделей\n",
    "Визуальная оценка генерируемых объектов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_images(ae_model_d2['model'].generate_samples(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_images(ae_model_d10['model'].generate_samples(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Визуализация латентного пространства (с точки зрения декодера)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_manifold_ae(model):\n",
    "    generator = lambda z: model.decode(torch.from_numpy(z).float()).view(28, 28).data.numpy()\n",
    "    return draw_manifold(generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_manifold_ae(ae_model_d2['model'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Визуализация латентного пространства (с точки зрения энкодера)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_latent_space(test_data.tensors[0][::10], test_data.tensors[1][::10],\n",
    "                  lambda data: ae_model_d2['model'].encode(data).detach())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ae_encoder_d10 = lambda data: TSNE().fit_transform(ae_model_d10['model'].encode(data).data.numpy())\n",
    "draw_latent_space(test_data.tensors[0][::25], test_data.tensors[1][::25], ae_encoder_d10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Автокодировщик: теперь вариационный!\n",
    "\n",
    "В качестве функции потерь используем бинарную кроссэнтропию.\n",
    "Это означает, что мы предполагаем, что каждый пискель - бинарная случайная величина.\n",
    "Генеративная сеть выдает вероятность каждого пикселя быть равным $1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models import log_likelihood, log_mean_exp, kl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models import VAE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Оценки функции потерь"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models import gaussian_log_pdf, compute_log_likelihood_monte_carlo, compute_log_likelihood_iwae"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Обучение модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vae_tests = [\n",
    "    {\n",
    "        'name': 'MC',\n",
    "        'func': lambda model:\n",
    "                model_test_loss(lambda batch:\n",
    "                                compute_log_likelihood_monte_carlo(batch, model, K=10),\n",
    "                                max_batches=20)\n",
    "    },\n",
    "    {\n",
    "        'name': 'IS',\n",
    "        'func': lambda model:\n",
    "                model_test_loss(lambda batch:\n",
    "                                compute_log_likelihood_iwae(batch, model, K=10),\n",
    "                                max_batches=20)\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vae_model_d2 = train_model(VAE(2, 784), tests=vae_tests, num_epochs=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vae_model_d10 = train_model(VAE(10, 784), tests=vae_tests, num_epochs=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(vae_model_d10['model'].state_dict(), 'vae_d10.model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Оценка качества модели\n",
    "\n",
    "Визуальная оценка генерируемых объектов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_images(vae_model_d2['model'].generate_samples(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_images(vae_model_d10['model'].generate_samples(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Визуализация латентного пространства (с точки зрения декодера)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_manifold_vae(model):\n",
    "    generator = lambda z: model.generative_distr(torch.from_numpy(z).float()).view(28, 28).data.numpy()\n",
    "    return draw_manifold(generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_manifold_vae(vae_model_d2['model'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Визуализация латетного пространства (с точки зрения энкодера)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vae_encoder = lambda data, model: model.sample_latent(model.proposal_distr(data))[:, 0].detach()\n",
    "draw_latent_space(test_data.tensors[0][::10], test_data.tensors[1][::10],\n",
    "                  lambda data: vae_encoder(data, vae_model_d2['model']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vae_encoder_d10 = lambda data: TSNE().fit_transform(vae_encoder(data, vae_model_d10['model']).data.numpy())\n",
    "draw_latent_space(test_data.tensors[0][::25], test_data.tensors[1][::25], vae_encoder_d10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Оценки логарифма правдоподобия на тестовых данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(9, 6))\n",
    "for label, name, model in [\n",
    "    ('VAE, Monte-Carlo, $d = 2$', 'MC', vae_model_d2),\n",
    "    ('VAE, Monte-Carlo, $d = 10$', 'MC', vae_model_d10),\n",
    "    ('VAE, Importance Sampling, $d = 2$', 'IS', vae_model_d2),\n",
    "    ('VAE, Importance Sampling, $d = 10$', 'IS', vae_model_d10),\n",
    "]:\n",
    "    data = [x[name] for x in model['test_results']]\n",
    "    x_labels = (1 + np.arange(len(data))) / 6\n",
    "    plt.plot(x_labels, data, label=label)\n",
    "plt.xlabel('Epoch')\n",
    "plt.xlim(xmax=x_labels[-1])\n",
    "plt.ylabel('Log-likelihood estimation, 10 samples')\n",
    "plt.legend()\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_results = []\n",
    "for K in [1, 5, 10, 50, 100, 500, 1000]:\n",
    "    print('K =', K, flush=True)\n",
    "    vae_tests_sampling = [\n",
    "        {\n",
    "            'name': 'D10MC',\n",
    "            'func': model_test_loss(lambda batch:\n",
    "                                    compute_log_likelihood_monte_carlo(batch, vae_model_d10['model'], K=K),\n",
    "                                    batch_size=10,\n",
    "                                    max_batches=50)\n",
    "        },\n",
    "        {\n",
    "            'name': 'D10IS',\n",
    "            'func': model_test_loss(lambda batch:\n",
    "                                    compute_log_likelihood_iwae(batch, vae_model_d10['model'], K=K),\n",
    "                                    batch_size=10,\n",
    "                                    max_batches=50)\n",
    "        },\n",
    "        {\n",
    "            'name': 'D2MC',\n",
    "            'func': model_test_loss(lambda batch:\n",
    "                                    compute_log_likelihood_monte_carlo(batch, vae_model_d2['model'], K=K),\n",
    "                                    batch_size=10,\n",
    "                                    max_batches=50)\n",
    "        },\n",
    "        {\n",
    "            'name': 'D2IS',\n",
    "            'func': model_test_loss(lambda batch:\n",
    "                                    compute_log_likelihood_iwae(batch, vae_model_d2['model'], K=K),\n",
    "                                    batch_size=10,\n",
    "                                    max_batches=50)\n",
    "        }\n",
    "    ]\n",
    "    cur_test_results = {'K': K}\n",
    "    for test in vae_tests_sampling:\n",
    "        cur_test_results[test['name']] = test['func']\n",
    "    test_results.append(cur_test_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(9, 6))\n",
    "for label, name in [\n",
    "    ('VAE, Monte-Carlo, $d = 2$', 'D2MC'),\n",
    "    ('VAE, Importance Sampling, $d = 2$', 'D2IWAE'),\n",
    "    ('VAE, Monte-Carlo, $d = 10$', 'D10MC'),\n",
    "    ('VAE, Importance Sampling, $d = 10$', 'D10IWAE'),\n",
    "]:\n",
    "    data = [x[name] for x in test_results]\n",
    "    x_labels = [x['K'] for x in test_results]\n",
    "    plt.plot(x_labels, data, label=label)\n",
    "plt.xlabel('Number of samples')\n",
    "plt.xscale('log')\n",
    "plt.ylabel('Log-likelihood estimation')\n",
    "plt.legend()\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Выводы"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Место для ваших выводов, наблюдений, гипотез."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
