{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![img](https://upload.wikimedia.org/wikipedia/commons/9/96/Pytorch_logo.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "% matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm, tqdm_notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use not only basic functionlaity of `pytorch` but also **`torchvision`** computer vision library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch==1.0.1.post2\r\n",
      "torchvision==0.2.1\r\n"
     ]
    }
   ],
   "source": [
    "!pip freeze 2>/dev/null | grep torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pytorch as a constructor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working with datasets\n",
    "\n",
    "For data loading pytorch defines **`Dataset`** entity.\n",
    "\n",
    "This abstract class is defined in `torch.utils.data.dataset`:\n",
    "\n",
    "```python\n",
    "class Dataset(object):\n",
    "    \"\"\"An abstract class representing a Dataset.\n",
    "\n",
    "    All other datasets should subclass it. All subclasses should override\n",
    "    ``__len__``, that provides the size of the dataset, and ``__getitem__``,\n",
    "    supporting integer indexing in range from 0 to len(self) exclusive.\n",
    "    \"\"\"\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def __len__(self):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def __add__(self, other):\n",
    "        return ConcatDataset([self, other])\n",
    "```\n",
    "One should inherit `Dataset` and implement `__getitem__` and `__len__` nethods to create a new data source.\n",
    "\n",
    "An example of such ancestor — `torchvision.datasets.ImageFolder`, which allows us to use imagenet-like dataset based on a directory with `./train/{class}` and `./val/{class}` sub-directories structure:\n",
    "\n",
    "```python\n",
    "imagenet = torchvision.datasets.ImageFolder('path/to/imagenet_root/')\n",
    "```\n",
    "\n",
    "Custom example — a dataset loading images with classes defined in some text file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "# default_loader - default image loading function, uses accimage or PIL\n",
    "from torchvision.datasets.folder import default_loader\n",
    "\n",
    "class TxtList(Dataset):\n",
    "    def __init__(self, path, transform=None, loader=default_loader):\n",
    "        with open(path) as fin:\n",
    "            self.imgs = [s.strip().split() for s in fin.readlines()]\n",
    "\n",
    "        print(f'=> Found {len(self.imgs)} entries in {path}')\n",
    "\n",
    "        self.classes = sorted(set([_[1] for _ in self.imgs]))\n",
    "        self.class_to_idx = {cls: idx for idx, cls in enumerate(self.classes)}\n",
    "        self.transform = transform\n",
    "        self.loader = loader\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        path, target = self.imgs[index]\n",
    "        target = self.class_to_idx[target]\n",
    "        img = self.loader(path)\n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)\n",
    "\n",
    "        return img, target\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "!echo '/tmp/1.jpg\\tcat' > /tmp/dataset.tsv\n",
    "!echo '/tmp/2.jpg\\tcat' >> /tmp/dataset.tsv\n",
    "!echo '/tmp/3.jpg\\tdog' >> /tmp/dataset.tsv\n",
    "!echo '/tmp/4.jpg\\tcat' >> /tmp/dataset.tsv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Found 4 entries in /tmp/dataset.tsv\n"
     ]
    }
   ],
   "source": [
    "catdog = TxtList('/tmp/dataset.tsv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['cat', 'dog']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "catdog.classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['/tmp/1.jpg', 'cat'],\n",
       " ['/tmp/2.jpg', 'cat'],\n",
       " ['/tmp/3.jpg', 'dog'],\n",
       " ['/tmp/4.jpg', 'cat']]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "catdog.imgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(catdog)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/tmp/1.jpg'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-09c24881b9b8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# FileNotFoundError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mcatdog\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-3-eb80aa5ad2c9>\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimgs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclass_to_idx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m         \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m             \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.anaconda3/lib/python3.6/site-packages/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36mdefault_loader\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m    145\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0maccimage_loader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 147\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mpil_loader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    148\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.anaconda3/lib/python3.6/site-packages/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36mpil_loader\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m    126\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mpil_loader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m     \u001b[0;31m# open path as file to avoid ResourceWarning (https://github.com/python-pillow/Pillow/issues/835)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 128\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    129\u001b[0m         \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'RGB'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/tmp/1.jpg'"
     ]
    }
   ],
   "source": [
    "# FileNotFoundError\n",
    "catdog[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`torchvision` has another useful classes for using standard datasets: \n",
    "http://pytorch.org/docs/master/torchvision/datasets.html.\n",
    "\n",
    "Some of them can be preloaded with built-in functionality, for example **MNIST**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "![ -d '/tmp/mnist/' ] && rm -r '/tmp/mnist/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Processing...\n",
      "Done!\n",
      "CPU times: user 246 ms, sys: 173 ms, total: 419 ms\n",
      "Wall time: 4.12 s\n"
     ]
    }
   ],
   "source": [
    "from torchvision.datasets import MNIST\n",
    "\n",
    "%time mnist = MNIST('/tmp/mnist/', train=True, download=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60000"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(mnist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(5)\n"
     ]
    }
   ],
   "source": [
    "image, target = mnist[0]\n",
    "print(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvOIA7rQAADgdJREFUeJzt3X9sXfV5x/HPs9D8QRoIXjUTpWFpIhQUIuZOJkwoGkXM5YeCggGhWkLKRBT3j1ii0hQNZX8MNAVFg2RqBKrsqqHJ1KWZBCghqpp0CZBOTBEmhF9mKQylqi2TFAWTH/zIHD/74x53Lvh+r3Pvufdc+3m/JMv3nuecex4d5ZPz8/pr7i4A8fxJ0Q0AKAbhB4Ii/EBQhB8IivADQRF+ICjCDwRF+IGgCD8Q1GWNXJmZ8TghUGfublOZr6Y9v5ndYWbHzex9M3ukls8C0FhW7bP9ZjZL0m8kdUgalPSqpC53H0gsw54fqLNG7PlXSHrf3T9w9wuSfi5pdQ2fB6CBagn/Akm/m/B+MJv2R8ys28z6zay/hnUByFndL/i5e5+kPonDfqCZ1LLnH5K0cML7b2bTAEwDtYT/VUnXmtm3zGy2pO9J2ptPWwDqrerDfncfNbMeSfslzZK03d3fya0zAHVV9a2+qlbGOT9Qdw15yAfA9EX4gaAIPxAU4QeCIvxAUIQfCIrwA0ERfiAowg8ERfiBoAg/EBThB4Ii/EBQhB8IivADQRF+ICjCDwRF+IGgCD8QFOEHgiL8QFCEHwiK8ANBEX4gKMIPBEX4gaAIPxAU4QeCIvxAUFUP0S1JZnZC0llJFyWNunt7Hk0hP7NmzUrWr7zyyrquv6enp2zt8ssvTy67dOnSZH39+vXJ+pNPPlm21tXVlVz2888/T9Y3b96crD/22GPJejOoKfyZW939oxw+B0ADcdgPBFVr+F3SATN7zcy682gIQGPUeti/0t2HzOzPJP3KzP7b3Q9PnCH7T4H/GIAmU9Oe392Hst+nJD0vacUk8/S5ezsXA4HmUnX4zWyOmc0dfy3pu5LezqsxAPVVy2F/q6TnzWz8c/7N3X+ZS1cA6q7q8Lv7B5L+IsdeZqxrrrkmWZ89e3ayfvPNNyfrK1euLFubN29ectn77rsvWS/S4OBgsr5t27ZkvbOzs2zt7NmzyWXfeOONZP3ll19O1qcDbvUBQRF+ICjCDwRF+IGgCD8QFOEHgjJ3b9zKzBq3sgZqa2tL1g8dOpSs1/trtc1qbGwsWX/ooYeS9XPnzlW97uHh4WT9448/TtaPHz9e9brrzd1tKvOx5weCIvxAUIQfCIrwA0ERfiAowg8ERfiBoLjPn4OWlpZk/ciRI8n64sWL82wnV5V6HxkZSdZvvfXWsrULFy4kl436/EOtuM8PIInwA0ERfiAowg8ERfiBoAg/EBThB4LKY5Te8E6fPp2sb9iwIVlftWpVsv76668n65X+hHXKsWPHkvWOjo5k/fz588n69ddfX7b28MMPJ5dFfbHnB4Ii/EBQhB8IivADQRF+ICjCDwRF+IGgKn6f38y2S1ol6ZS7L8+mtUjaLWmRpBOSHnD39B8618z9Pn+trrjiimS90nDSvb29ZWtr165NLvvggw8m67t27UrW0Xzy/D7/TyXd8aVpj0g66O7XSjqYvQcwjVQMv7sflvTlR9hWS9qRvd4h6Z6c+wJQZ9We87e6+/h4Rx9Kas2pHwANUvOz/e7uqXN5M+uW1F3regDkq9o9/0kzmy9J2e9T5WZ09z53b3f39irXBaAOqg3/XklrstdrJO3Jpx0AjVIx/Ga2S9J/SVpqZoNmtlbSZkkdZvaepL/J3gOYRiqe87t7V5nSbTn3EtaZM2dqWv6TTz6petl169Yl67t3707Wx8bGql43isUTfkBQhB8IivADQRF+ICjCDwRF+IGgGKJ7BpgzZ07Z2gsvvJBc9pZbbknW77zzzmT9wIEDyToajyG6ASQRfiAowg8ERfiBoAg/EBThB4Ii/EBQ3Oef4ZYsWZKsHz16NFkfGRlJ1l988cVkvb+/v2zt6aefTi7byH+bMwn3+QEkEX4gKMIPBEX4gaAIPxAU4QeCIvxAUNznD66zszNZf+aZZ5L1uXPnVr3ujRs3Jus7d+5M1oeHh5P1qLjPDyCJ8ANBEX4gKMIPBEX4gaAIPxAU4QeCqnif38y2S1ol6ZS7L8+mPSppnaTfZ7NtdPdfVFwZ9/mnneXLlyfrW7duTdZvu636kdx7e3uT9U2bNiXrQ0NDVa97OsvzPv9PJd0xyfR/cfe27Kdi8AE0l4rhd/fDkk43oBcADVTLOX+Pmb1pZtvN7KrcOgLQENWG/0eSlkhqkzQsaUu5Gc2s28z6zaz8H3MD0HBVhd/dT7r7RXcfk/RjSSsS8/a5e7u7t1fbJID8VRV+M5s/4W2npLfzaQdAo1xWaQYz2yXpO5K+YWaDkv5R0nfMrE2SSzoh6ft17BFAHfB9ftRk3rx5yfrdd99dtlbpbwWYpW9XHzp0KFnv6OhI1mcqvs8PIInwA0ERfiAowg8ERfiBoAg/EBS3+lCYL774Ilm/7LL0Yyijo6PJ+u2331629tJLLyWXnc641QcgifADQRF+ICjCDwRF+IGgCD8QFOEHgqr4fX7EdsMNNyTr999/f7J+4403lq1Vuo9fycDAQLJ++PDhmj5/pmPPDwRF+IGgCD8QFOEHgiL8QFCEHwiK8ANBcZ9/hlu6dGmy3tPTk6zfe++9yfrVV199yT1N1cWLF5P14eHhZH1sbCzPdmYc9vxAUIQfCIrwA0ERfiAowg8ERfiBoAg/EFTF+/xmtlDSTkmtklxSn7v/0MxaJO2WtEjSCUkPuPvH9Ws1rkr30ru6usrWKt3HX7RoUTUt5aK/vz9Z37RpU7K+d+/ePNsJZyp7/lFJf+fuyyT9laT1ZrZM0iOSDrr7tZIOZu8BTBMVw+/uw+5+NHt9VtK7khZIWi1pRzbbDkn31KtJAPm7pHN+M1sk6duSjkhqdffx5ys/VOm0AMA0MeVn+83s65KelfQDdz9j9v/Dgbm7lxuHz8y6JXXX2iiAfE1pz29mX1Mp+D9z9+eyySfNbH5Wny/p1GTLunufu7e7e3seDQPIR8XwW2kX/xNJ77r71gmlvZLWZK/XSNqTf3sA6qXiEN1mtlLSryW9JWn8O5IbVTrv/3dJ10j6rUq3+k5X+KyQQ3S3tqYvhyxbtixZf+qpp5L166677pJ7ysuRI0eS9SeeeKJsbc+e9P6Cr+RWZ6pDdFc853f3/5RU7sNuu5SmADQPnvADgiL8QFCEHwiK8ANBEX4gKMIPBMWf7p6ilpaWsrXe3t7ksm1tbcn64sWLq+opD6+88kqyvmXLlmR9//79yfpnn312yT2hMdjzA0ERfiAowg8ERfiBoAg/EBThB4Ii/EBQYe7z33TTTcn6hg0bkvUVK1aUrS1YsKCqnvLy6aeflq1t27Ytuezjjz+erJ8/f76qntD82PMDQRF+ICjCDwRF+IGgCD8QFOEHgiL8QFBh7vN3dnbWVK/FwMBAsr5v375kfXR0NFlPfed+ZGQkuSziYs8PBEX4gaAIPxAU4QeCIvxAUIQfCIrwA0GZu6dnMFsoaaekVkkuqc/df2hmj0paJ+n32awb3f0XFT4rvTIANXN3m8p8Uwn/fEnz3f2omc2V9JqkeyQ9IOmcuz851aYIP1B/Uw1/xSf83H1Y0nD2+qyZvSup2D9dA6Bml3TOb2aLJH1b0pFsUo+ZvWlm283sqjLLdJtZv5n119QpgFxVPOz/w4xmX5f0sqRN7v6cmbVK+kil6wD/pNKpwUMVPoPDfqDOcjvnlyQz+5qkfZL2u/vWSeqLJO1z9+UVPofwA3U21fBXPOw3M5P0E0nvTgx+diFwXKekty+1SQDFmcrV/pWSfi3pLUlj2eSNkroktal02H9C0vezi4Opz2LPD9RZrof9eSH8QP3ldtgPYGYi/EBQhB8IivADQRF+ICjCDwRF+IGgCD8QFOEHgiL8QFCEHwiK8ANBEX4gKMIPBNXoIbo/kvTbCe+/kU1rRs3aW7P2JdFbtfLs7c+nOmNDv8//lZWb9bt7e2ENJDRrb83al0Rv1SqqNw77gaAIPxBU0eHvK3j9Kc3aW7P2JdFbtQrprdBzfgDFKXrPD6AghYTfzO4ws+Nm9r6ZPVJED+WY2Qkze8vMjhU9xFg2DNopM3t7wrQWM/uVmb2X/Z50mLSCenvUzIaybXfMzO4qqLeFZvaimQ2Y2Ttm9nA2vdBtl+irkO3W8MN+M5sl6TeSOiQNSnpVUpe7DzS0kTLM7ISkdncv/J6wmf21pHOSdo6PhmRm/yzptLtvzv7jvMrd/75JentUlzhyc516Kzey9N+qwG2X54jXeShiz79C0vvu/oG7X5D0c0mrC+ij6bn7YUmnvzR5taQd2esdKv3jabgyvTUFdx9296PZ67OSxkeWLnTbJfoqRBHhXyDpdxPeD6q5hvx2SQfM7DUz6y66mUm0ThgZ6UNJrUU2M4mKIzc30pdGlm6abVfNiNd544LfV61097+UdKek9dnhbVPy0jlbM92u+ZGkJSoN4zYsaUuRzWQjSz8r6QfufmZirchtN0lfhWy3IsI/JGnhhPffzKY1BXcfyn6fkvS8SqcpzeTk+CCp2e9TBffzB+5+0t0vuvuYpB+rwG2XjSz9rKSfuftz2eTCt91kfRW13YoI/6uSrjWzb5nZbEnfk7S3gD6+wszmZBdiZGZzJH1XzTf68F5Ja7LXayTtKbCXP9IsIzeXG1laBW+7phvx2t0b/iPpLpWu+P+PpH8ooocyfS2W9Eb2807RvUnapdJh4P+qdG1kraQ/lXRQ0nuS/kNSSxP19q8qjeb8pkpBm19QbytVOqR/U9Kx7Oeuorddoq9CthtP+AFBccEPCIrwA0ERfiAowg8ERfiBoAg/EBThB4Ii/EBQ/weCC5r/92q6mAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(np.array(image), 'gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='#cc6666'>Hometask!</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement **`UrlList`** dataset which costructor takes list of urls as a parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "\n",
    "class UrlList(Dataset):\n",
    "    def __init__(self, urls: List[str]):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Demonstrate how it works with some examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data transformation\n",
    "\n",
    "In the example shown before and in built-in `ImageFolder` `__init__` methods has `transform` parameter (and `target_transform`).\n",
    "\n",
    "They are used to transform images/targets loaded into predefined range and form.\n",
    "\n",
    "There is `transforms` sub-module in `torchvision` library which has some examples of such transforms:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example `transforms.ToTensor()` transforms uint8 `PIL` [0, 256)-domained images into [0, 1)-domained tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_tensor = transforms.ToTensor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 28, 28]), tensor(0.), tensor(1.))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pil_image = mnist[0][0]\n",
    "th_image = to_tensor(pil_image)\n",
    "th_image.shape, th_image.min(), th_image.max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One can define `normalize` to implement a standard ImageNet preparation step:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`transforms.Compose` is used to sequence several compositions as a whole:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "crop_and_tensorize = transforms.Compose([\n",
    "    transforms.CenterCrop(16),\n",
    "    transforms.ToTensor(),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(crop_and_tensorize(pil_image)[0].numpy(), 'gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To define a custom transformer we should only implement `__call__` method its implementation:\n",
    "\n",
    "```python\n",
    "class HorizontalFlip(object):\n",
    "    def __init__(self, mode=0):\n",
    "        self.method = mode\n",
    "\n",
    "    def __call__(self, img):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            img (PIL.Image): Image to be flipped.\n",
    "\n",
    "        Returns:\n",
    "            PIL.Image: Randomly flipped image.\n",
    "        \"\"\"\n",
    "        if self.method:\n",
    "            return img.transpose(Image.FLIP_LEFT_RIGHT)\n",
    "        return img\n",
    "\n",
    "```\n",
    "\n",
    "It is adviced to take a glance at http://pytorch.org/docs/master/torchvision/transforms.html for standard transformations overview."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optional reading\n",
    "\n",
    "Another example of a good set of pre-defined transformations is [`albumentations`](https://github.com/albu/albumentations) library.\n",
    "\n",
    "It supports not only image transformations but it can simultaniously transform its corresponding masks and bboxes.\n",
    "\n",
    "One can also use [`imgaug`](https://github.com/aleju/imgaug) augmentations library but needs to implement `imguag output` -> `tensor` transformations on their own."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='#cc6666'>Hometask!</font>\n",
    "\n",
    "Implement a transformer that applies random transformation from $D_4$ transformations group.\n",
    "\n",
    "These transformations are very usefull for lossless augmentations in satellite images analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomD4(object):\n",
    "    def __call__(self, img):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Demonstrate how it works on some MNIST images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data loaders\n",
    "\n",
    "The main reason to implement `Dataset` class is a magic power of `torchvision` loaders:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The loaders are built on top of some dataset and allow batch iterating over it.\n",
    "\n",
    "Those batches in their turn are created with applying the transformations defined in background processes.\n",
    "\n",
    "Let's look at MNIST dataset with a simple transformer applied as loaders use tensors and not `PIL.Image`s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed_mnist = MNIST('/tmp/mnist/', train=True, transform=transforms.ToTensor())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_loader = DataLoader(transformed_mnist, batch_size=16, shuffle=True, num_workers=4)  # shuffle note here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DataLoader standard parameter values are shown above.\n",
    "\n",
    "One can also add **`pin_memory=True`** to page-lock the memory thus making faster cpu-to-cuda transfer with a non-blocking option.\n",
    "\n",
    "**`drop_last=True`** is for avoiding batches size skew in the training process.\n",
    "\n",
    "Let us demonstrate how it works:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3750/3750 [00:01<00:00, 2058.27it/s]\n"
     ]
    }
   ],
   "source": [
    "for images, targets in tqdm(mnist_loader):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are image batches as tensors on the output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 1, 28, 28])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targets.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The loader instance pre-calculate the total number of batches (what makes `tqdm` happy):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3750"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(mnist_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some data example (will be random on each run):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(images[0][0].numpy(), 'gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2)\n"
     ]
    }
   ],
   "source": [
    "print(targets[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building NN models\n",
    "\n",
    "We should inherit `torch.nn.Module` class to implement pytorch model, `torch.nn` has lots of pre-defined \"building bricks\".\n",
    "\n",
    "Their functional analogs are collected in `torch.nn.functional`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have both module and functional versions of max-pooling, activations, upsempling and some other ops:\n",
    "* `nn.MaxPool2d` / `F.max_pool2d`\n",
    "* `nn.ReLU` / `F.relu`\n",
    "* `nn.Upsample(mode='bilinar')` / `F.upsample(mode='bilinar')` — **deprecated**, use `F.interpolate` instead\n",
    "\n",
    "To compose several nn modules we can use `nn.Squential`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (1): ReLU()\n",
      "  (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (3): ReLU()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "layers = [\n",
    "    nn.Conv2d(3, 64, 3, padding=1),\n",
    "    nn.ReLU(),\n",
    "    nn.Conv2d(64, 64, 3, padding=1),\n",
    "    nn.ReLU()\n",
    "]\n",
    "unet_down1 = nn.Sequential(*layers)\n",
    "print(unet_down1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models training\n",
    "\n",
    "To train a model one must define a looss functions, some of them can also be found in `torch.nn`:\n",
    "\n",
    "```python\n",
    "output = model(torch.cat(x, 1))\n",
    "target = torch.arange(1, 1001)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "loss = criterion(output, target)\n",
    "print(loss)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can update the weights manually:\n",
    "\n",
    "```python\n",
    "learning_rate = 0.01\n",
    "for f in net.parameters():\n",
    "    f.data.sub_(f.grad.data * learning_rate)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or use built-in optimizer from `torch.optim` family:\n",
    "```python\n",
    "import torch.optim as optim\n",
    "\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.01)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to zero the gradients to disable gradient accumulations that happens by defult (`pytorch` sends its regards to `tf` rnn implementations):\n",
    "```python\n",
    "optimizer.zero_grad()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After zeroing the gradients we can run both forward and backward stage with loss calculation in-between:\n",
    "```python\n",
    "output = net(x)\n",
    "loss = criterion(output, target)\n",
    "loss.backward()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To update the weights we can use the optimizer again:\n",
    "```python\n",
    "optimizer.step()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To adjust learning rate in some pre-defined way we can use **`torch.optim.lr_scheduler`** sub-module:\n",
    "```python\n",
    "from torch.optim import lr_scheduler\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To train ResNets on ImageNet with a standard training scheme we can use\n",
    "```python\n",
    "scheduler = lr_scheduler.StepLR(optimizer, step_size=30, gamma=0.1)\n",
    "for epoch in range(100):\n",
    "     scheduler.step()  # == scheduler.step(epoch)\n",
    "     # train(...)\n",
    "     # validate(...)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optional reading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are several libraries out there like [`catalyst`](https://github.com/catalyst-team/catalyst) or [`kekas`](https://github.com/belskikh/kekas) that make building pipelines even easier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pytorch in the wils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Segmentation model creation (with U-Net as an example)\n",
    "\n",
    "![img](https://lmb.informatik.uni-freiburg.de/people/ronneber/u-net/u-net-architecture.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pre-defined heavily-used convolutions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv3x3(in_channels, out_channels, dilation=1):\n",
    "    return nn.Conv2d(in_channels, out_channels, 3, padding=dilation, dilation=dilation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One **encoder block** consists of two sequential convolutions, an activation layer and an optional batch-norm:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, batch_norm=False):\n",
    "        super().__init__()\n",
    "\n",
    "        self.batch_norm = batch_norm\n",
    "\n",
    "        self.conv1 = conv3x3(in_channels, out_channels)\n",
    "        if self.batch_norm:\n",
    "            self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.conv2 = conv3x3(out_channels, out_channels)\n",
    "        if self.batch_norm:\n",
    "            self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "        self.relu2 = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        if self.batch_norm:\n",
    "            x = self.bn1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.conv2(x)\n",
    "        if self.batch_norm:\n",
    "            x = self.bn2(x)\n",
    "        x = self.relu2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EncoderBlock(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (relu1): ReLU()\n",
       "  (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (relu2): ReLU()\n",
       ")"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "block = EncoderBlock(3, 64)\n",
    "block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 64, 128, 128])\n"
     ]
    }
   ],
   "source": [
    "x = torch.zeros(4, 3, 128, 128)\n",
    "\n",
    "with torch.no_grad():\n",
    "    print(block(x).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternative definition:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, batch_norm=False):\n",
    "        super().__init__()\n",
    "\n",
    "        self.block = nn.Sequential()\n",
    "        self.block.add_module('conv1', conv3x3(in_channels, out_channels))\n",
    "        if batch_norm:\n",
    "            self.block.add_module('bn1', nn.BatchNorm2d(out_channels))\n",
    "        self.block.add_module('relu1', nn.ReLU())\n",
    "        self.block.add_module('conv2', conv3x3(out_channels, out_channels))\n",
    "        if batch_norm:\n",
    "            self.block.add_module('bn2', nn.BatchNorm2d(out_channels))\n",
    "        self.block.add_module('relu2', nn.ReLU())\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.block(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EncoderBlock(\n",
       "  (block): Sequential(\n",
       "    (conv1): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (relu1): ReLU()\n",
       "    (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (relu2): ReLU()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "block = EncoderBlock(3, 64)\n",
    "block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 64, 128, 128])\n"
     ]
    }
   ],
   "source": [
    "x = torch.zeros(4, 3, 128, 128)\n",
    "\n",
    "with torch.no_grad():\n",
    "    print(block(x).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And its \"functional analog\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encoder_block(in_channels, out_channels, batch_norm=False):\n",
    "    block = nn.Sequential()\n",
    "    block.add_module('conv1', conv3x3(in_channels, out_channels))\n",
    "    if batch_norm:\n",
    "        block.add_module('bn1', nn.BatchNorm2d(out_channels))\n",
    "    block.add_module('relu1', nn.ReLU())\n",
    "    block.add_module('conv2', conv3x3(out_channels, out_channels))\n",
    "    if batch_norm:\n",
    "        block.add_module('bn2', nn.BatchNorm2d(out_channels))\n",
    "    block.add_module('relu2', nn.ReLU())\n",
    "    return block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (relu1): ReLU()\n",
       "  (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (relu2): ReLU()\n",
       ")"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "block = encoder_block(3, 64)\n",
    "block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 64, 128, 128])\n"
     ]
    }
   ],
   "source": [
    "x = torch.zeros(4, 3, 128, 128)\n",
    "\n",
    "with torch.no_grad():\n",
    "    print(block(x).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Encoder is composed from several encoder blocks.\n",
    "\n",
    "Its final form is defined bu the number of input channels, the number of channels in the first block output and the number of such blocks.\n",
    "\n",
    "And we need to store preliminary activatons to apply Decoder on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, in_channels, num_filters, num_blocks):\n",
    "        super().__init__()\n",
    "\n",
    "        self.num_blocks = num_blocks\n",
    "        for i in range(num_blocks):\n",
    "            in_channels = in_channels if not i else num_filters * 2 ** (i - 1)\n",
    "            out_channels = num_filters * 2**i\n",
    "            self.add_module(f'block{i + 1}', encoder_block(in_channels, out_channels))\n",
    "            if i != num_blocks - 1:\n",
    "                self.add_module(f'pool{i + 1}', nn.MaxPool2d(2, 2))\n",
    "\n",
    "    def forward(self, x):\n",
    "        acts = []\n",
    "        for i in range(self.num_blocks):\n",
    "            x = self.__getattr__(f'block{i + 1}')(x)\n",
    "            acts.append(x)\n",
    "            if i != self.num_blocks - 1:\n",
    "                x = self.__getattr__(f'pool{i + 1}')(x)\n",
    "        return acts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we can use **`add_module`** way of layers definition as its number is variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Encoder(\n",
       "  (block1): Sequential(\n",
       "    (conv1): Conv2d(3, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (relu1): ReLU()\n",
       "    (conv2): Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (relu2): ReLU()\n",
       "  )\n",
       "  (pool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (block2): Sequential(\n",
       "    (conv1): Conv2d(8, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (relu1): ReLU()\n",
       "    (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (relu2): ReLU()\n",
       "  )\n",
       "  (pool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (block3): Sequential(\n",
       "    (conv1): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (relu1): ReLU()\n",
       "    (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (relu2): ReLU()\n",
       "  )\n",
       "  (pool3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (block4): Sequential(\n",
       "    (conv1): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (relu1): ReLU()\n",
       "    (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (relu2): ReLU()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder = Encoder(in_channels=3, num_filters=8, num_blocks=4)\n",
    "encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[torch.Size([4, 8, 512, 512]),\n",
       " torch.Size([4, 16, 256, 256]),\n",
       " torch.Size([4, 32, 128, 128]),\n",
       " torch.Size([4, 64, 64, 64])]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.zeros(4, 3, 512, 512)\n",
    "\n",
    "[_.shape for _ in encoder(x)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decoder block consists of upscaling \"lower\" output and concatenating it with a saved encoder block output from the \"left\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "Upsample = nn.Upsample\n",
    "\n",
    "# class Upsample(nn.Module):\n",
    "#     def __init__(self, scale_factor=2, mode='bilinear'):\n",
    "#         super().__init__()\n",
    "\n",
    "#         self.scale_factor = scale_factor\n",
    "#         self.mode = mode\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         return F.interpolate(x, scale_factor=self.scale_factor, mode=self.mode, align_corners=True)\n",
    "\n",
    "class DecoderBlock(nn.Module):\n",
    "    def __init__(self, out_channels):\n",
    "        super().__init__()\n",
    "\n",
    "        self.uppool = Upsample(scale_factor=2, mode='bilinear')\n",
    "        self.upconv = conv3x3(out_channels * 2, out_channels)\n",
    "        self.conv1 = conv3x3(out_channels * 2, out_channels)\n",
    "        self.conv2 = conv3x3(out_channels, out_channels)\n",
    "\n",
    "    def forward(self, down, left):\n",
    "        x = self.uppool(down)\n",
    "        x = self.upconv(x)\n",
    "        x = torch.cat([left, x], 1)\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "block = DecoderBlock(8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = encoder(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([4, 16, 256, 256]), torch.Size([4, 8, 512, 512]))"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y[1].shape, y[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nizhib/.anaconda3/lib/python3.6/site-packages/torch/nn/modules/upsampling.py:129: UserWarning: nn.Upsample is deprecated. Use nn.functional.interpolate instead.\n",
      "  warnings.warn(\"nn.{} is deprecated. Use nn.functional.interpolate instead.\".format(self.name))\n",
      "/home/nizhib/.anaconda3/lib/python3.6/site-packages/torch/nn/functional.py:2423: UserWarning: Default upsampling behavior when mode=bilinear is changed to align_corners=False since 0.4.0. Please specify align_corners=True if the old behavior is desired. See the documentation of nn.Upsample for details.\n",
      "  \"See the documentation of nn.Upsample for details.\".format(mode))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 512, 512])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "block(y[1], y[0]).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us build Decoder from several decoder blocks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, num_filters, num_blocks):\n",
    "        super().__init__()\n",
    "\n",
    "        for i in range(num_blocks):\n",
    "            self.add_module(f'block{num_blocks - i}', DecoderBlock(num_filters * 2**i))\n",
    "\n",
    "    def forward(self, acts):\n",
    "        up = acts[-1]\n",
    "        for i, left in enumerate(acts[-2::-1]):\n",
    "            up = self.__getattr__(f'block{i + 1}')(up, left)\n",
    "        return up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder = Decoder(8, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 3, 512, 512])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[torch.Size([4, 8, 512, 512]),\n",
       " torch.Size([4, 16, 256, 256]),\n",
       " torch.Size([4, 32, 128, 128]),\n",
       " torch.Size([4, 64, 64, 64])]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[_.shape for _ in encoder(x)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nizhib/.anaconda3/lib/python3.6/site-packages/torch/nn/modules/upsampling.py:129: UserWarning: nn.Upsample is deprecated. Use nn.functional.interpolate instead.\n",
      "  warnings.warn(\"nn.{} is deprecated. Use nn.functional.interpolate instead.\".format(self.name))\n",
      "/home/nizhib/.anaconda3/lib/python3.6/site-packages/torch/nn/functional.py:2423: UserWarning: Default upsampling behavior when mode=bilinear is changed to align_corners=False since 0.4.0. Please specify align_corners=True if the old behavior is desired. See the documentation of nn.Upsample for details.\n",
      "  \"See the documentation of nn.Upsample for details.\".format(mode))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 512, 512])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder(encoder(x)).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "U-Net is build from Encoder, Decoder and a final classification layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UNet(nn.Module):\n",
    "    def __init__(self, num_classes, in_channels=3, num_filters=64, num_blocks=4):\n",
    "        super().__init__()\n",
    "\n",
    "        print(f'=> Building {num_blocks}-blocks {num_filters}-filter U-Net')\n",
    "\n",
    "        self.encoder = Encoder(in_channels, num_filters, num_blocks)\n",
    "        self.decoder = Decoder(num_filters, num_blocks - 1)\n",
    "        self.final = nn.Conv2d(num_filters, num_classes, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        acts = self.encoder(x)\n",
    "        x = self.decoder(acts)\n",
    "        x = self.final(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"Integration testing\" (pytorch 0.3 legacy code):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Building 4-blocks 64-filter U-Net\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nizhib/.anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:7: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
      "  import sys\n",
      "/home/nizhib/.anaconda3/lib/python3.6/site-packages/torch/nn/modules/upsampling.py:129: UserWarning: nn.Upsample is deprecated. Use nn.functional.interpolate instead.\n",
      "  warnings.warn(\"nn.{} is deprecated. Use nn.functional.interpolate instead.\".format(self.name))\n",
      "/home/nizhib/.anaconda3/lib/python3.6/site-packages/torch/nn/functional.py:2423: UserWarning: Default upsampling behavior when mode=bilinear is changed to align_corners=False since 0.4.0. Please specify align_corners=True if the old behavior is desired. See the documentation of nn.Upsample for details.\n",
      "  \"See the documentation of nn.Upsample for details.\".format(mode))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 1, 416, 416])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch.autograd import Variable\n",
    "\n",
    "model = UNet(num_classes=1)\n",
    "if torch.cuda.is_available():\n",
    "    model.cuda()\n",
    "\n",
    "images = Variable(torch.randn(4, 3, 416, 416), volatile=True)\n",
    "if torch.cuda.is_available():\n",
    "    images = images.cuda()\n",
    "\n",
    "model.forward(images).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Same for **pytorch 0.4+**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Building 4-blocks 64-filter U-Net\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nizhib/.anaconda3/lib/python3.6/site-packages/torch/nn/modules/upsampling.py:129: UserWarning: nn.Upsample is deprecated. Use nn.functional.interpolate instead.\n",
      "  warnings.warn(\"nn.{} is deprecated. Use nn.functional.interpolate instead.\".format(self.name))\n",
      "/home/nizhib/.anaconda3/lib/python3.6/site-packages/torch/nn/functional.py:2423: UserWarning: Default upsampling behavior when mode=bilinear is changed to align_corners=False since 0.4.0. Please specify align_corners=True if the old behavior is desired. See the documentation of nn.Upsample for details.\n",
      "  \"See the documentation of nn.Upsample for details.\".format(mode))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 1, 416, 416])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "model = UNet(num_classes=1)\n",
    "model.to(device)\n",
    "\n",
    "images = torch.randn(4, 3, 416, 416).to(device)\n",
    "\n",
    "model.forward(images).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We get linear activations as an output.\n",
    "\n",
    "To train such models we need to use **/.\\*WithLogits/** loss functions subset\n",
    "\n",
    "We can use `torch.sigmoid` or `torch.softmax` to get probabilities (**0.4.1+**, `torch.nn.functional.sigmoid/softmax` before)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using pretrained encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Encoder blocks structure seen before looks exactly like VGG architecture:\n",
    "\n",
    "![img](https://www.pyimagesearch.com/wp-content/uploads/2017/03/imagenet_vgg16.png)\n",
    "\n",
    "Let us have a look at VGG model from `torchvision` library:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models import vgg13"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "VGG13 is the VGG version with 2 convolutional layer in each block:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = vgg13()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VGG(\n",
       "  (features): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU(inplace)\n",
       "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (3): ReLU(inplace)\n",
       "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (6): ReLU(inplace)\n",
       "    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (8): ReLU(inplace)\n",
       "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (11): ReLU(inplace)\n",
       "    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (13): ReLU(inplace)\n",
       "    (14): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (15): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (16): ReLU(inplace)\n",
       "    (17): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (18): ReLU(inplace)\n",
       "    (19): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (20): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (21): ReLU(inplace)\n",
       "    (22): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (23): ReLU(inplace)\n",
       "    (24): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (classifier): Sequential(\n",
       "    (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
       "    (1): ReLU(inplace)\n",
       "    (2): Dropout(p=0.5)\n",
       "    (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "    (4): ReLU(inplace)\n",
       "    (5): Dropout(p=0.5)\n",
       "    (6): Linear(in_features=4096, out_features=1000, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We don't need the classifier, only features are useful.\n",
    "\n",
    "They are built from conv-relu-conv-relu + maxpooling.\n",
    "\n",
    "Let's build encoder blocks by VGG layers grouping:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VGG13Encoder(nn.Module):\n",
    "    def __init__(self, num_blocks, pretrained=True):\n",
    "        super().__init__()\n",
    "\n",
    "        backbone = vgg13(pretrained=pretrained).features\n",
    "\n",
    "        self.num_blocks = num_blocks\n",
    "        for i in range(self.num_blocks):\n",
    "            block = nn.Sequential(*[backbone[j] for j in range(i * 5, i * 5 + 4)])\n",
    "            self.add_module(f'block{i + 1}', block)\n",
    "            if i != num_blocks - 1:\n",
    "                self.add_module(f'pool{i + 1}', nn.MaxPool2d(2, 2))\n",
    "\n",
    "    def forward(self, x):\n",
    "        acts = []\n",
    "        for i in range(self.num_blocks):\n",
    "            x = self.__getattr__(f'block{i + 1}')(x)\n",
    "            acts.append(x)\n",
    "            if i != self.num_blocks - 1:\n",
    "                x = self.__getattr__(f'pool{i + 1}')(x)\n",
    "        return acts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VGG13Encoder(\n",
       "  (block1): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU(inplace)\n",
       "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (3): ReLU(inplace)\n",
       "  )\n",
       "  (pool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (block2): Sequential(\n",
       "    (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU(inplace)\n",
       "    (2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (3): ReLU(inplace)\n",
       "  )\n",
       "  (pool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (block3): Sequential(\n",
       "    (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU(inplace)\n",
       "    (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (3): ReLU(inplace)\n",
       "  )\n",
       "  (pool3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (block4): Sequential(\n",
       "    (0): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU(inplace)\n",
       "    (2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (3): ReLU(inplace)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vgg_encoder = VGG13Encoder(num_blocks=4)\n",
    "vgg_encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare it to \"vanilla\" encoder:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Encoder(\n",
       "  (block1): Sequential(\n",
       "    (conv1): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (relu1): ReLU()\n",
       "    (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (relu2): ReLU()\n",
       "  )\n",
       "  (pool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (block2): Sequential(\n",
       "    (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (relu1): ReLU()\n",
       "    (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (relu2): ReLU()\n",
       "  )\n",
       "  (pool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (block3): Sequential(\n",
       "    (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (relu1): ReLU()\n",
       "    (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (relu2): ReLU()\n",
       "  )\n",
       "  (pool3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (block4): Sequential(\n",
       "    (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (relu1): ReLU()\n",
       "    (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (relu2): ReLU()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder = Encoder(in_channels=3, num_filters=64, num_blocks=4)\n",
    "encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both structures are identical!\n",
    "\n",
    "But now we have some usefull pretrained weights in the decoder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='#cc6666'>Hometask!</font>\n",
    "\n",
    "### Part1. Toy dataset\n",
    "\n",
    "\n",
    "**Implement** toy dataset to generate noisy ellipses like that:\n",
    "\n",
    "![img](https://raw.githubusercontent.com/jakeret/tf_unet/master/docs/toy_problem.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset should output both the image and its corresponding mask."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Ellipses(Dataset):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define utility function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hashlib\n",
    "\n",
    "def moy_variant(surname):\n",
    "    return int(hashlib.md5(surname.encode().lower()).hexdigest()[-1], 16) % 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Implement CCNet** (https://arxiv.org/abs/1811.11721, if `moy_variant` returns 1 for you) **or OCNet** (https://arxiv.org/abs/1809.00916, otherwise) based on pretrained **DenseNet** (pytorch) and **DPN** (cadene/pretrainedmodels) networks. It is advised to use `BCEWithLogitsLoss` as a loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models import *\n",
    "\n",
    "\n",
    "class CCDenseNet(nn.Module):\n",
    "    def __init__(self, num_classes, arch='densenet161'):\n",
    "        pass\n",
    "\n",
    "    def forward(self, x):\n",
    "        pass\n",
    "\n",
    "\n",
    "class CCDPN(nn.Module):\n",
    "    def __init__(self, num_classes, arch='dpn92'):\n",
    "        pass\n",
    "\n",
    "    def forward(self, x):\n",
    "        pass\n",
    "\n",
    "\n",
    "class OCDenseNet(nn.Module):\n",
    "    def __init__(self, num_classes, arch='densenet161'):\n",
    "        pass\n",
    "\n",
    "    def forward(self, x):\n",
    "        pass\n",
    "\n",
    "\n",
    "class OCDPN(nn.Module):\n",
    "    def __init__(self, num_classes, arch='dpn92'):\n",
    "        pass\n",
    "\n",
    "    def forward(self, x):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Demonstrate** how they are trained with `tensorboard` screenshots (use `tensorboardX` library).\n",
    "\n",
    "That includes but not limited to loss curves, masks from different epochs etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part2. Real problem\n",
    "\n",
    "**TBA**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 3. Bells and whistles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Use at least 5 `albumentations` or `imgaug` augmentations in the data preparation (1 bonus point).\n",
    "\n",
    "\n",
    "* Use `catalyst` or `kekas` to build a training pipeline (1 bonus point)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
