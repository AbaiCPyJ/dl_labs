{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Seminar 3. Loss surfaces (advanced task)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the active research fields in deep learning is study of loss surfaces. Such works focus on several questions:\n",
    "* What is the shape of the loss surface? Do loss optima look like \"basins of attraction\" (point optima)? Or do they form connected components at the bottom of the landscape?\n",
    "* Why do neural networks actually learn so well? Why do local optima found by SGD generalize well?\n",
    "* How does SGD traverse highly nonlinear, nonconvex loss surface?\n",
    "\n",
    "Today you will conduct several experiments following several recently published papers to shed the light on these questions.\n",
    "\n",
    "The task consists of three __independent__ parts. You may do them in any order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import lab_cnn_solution as code\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "import os\n",
    "import losssurf_lab_utils as utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Module lab_cnn_solution contains the solution for the basic part of the seminar (training CNN on MNIST). You will use this code. The function below trains a model using this code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# it is _highly_ recommended to do the task on GPU\n",
    "device = torch.device('cuda') # change to cpu if needed\n",
    "train_loader, test_loader = code.get_mnist()\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def learn_one_model(num_epochs = 10, path_to_save = None):\n",
    "    \"\"\"\n",
    "    learns 1 CNN on MNIST and returns learned model\n",
    "    :num_epochs: int\n",
    "    :path_to_save: if not None, \n",
    "                   weights after _each_ epoch are saved to the corresponding folder\n",
    "                   if None, weights are not saved\n",
    "    \"\"\"\n",
    "    if not os.path.exists(path_to_save):\n",
    "        os.mkdir(path_to_save)\n",
    "    cnn = code.CNN().to(device)\n",
    "    optimizer = torch.optim.Adam(cnn.parameters(), lr=0.001)\n",
    "    train_log, train_acc_log, val_log, val_acc_log = \\\n",
    "        code.train(cnn, optimizer, train_loader, test_loader, criterion, \\\n",
    "                   num_epochs, device, 0, path_to_save)\n",
    "    return cnn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Technical code__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Disclamer:__ you will use this code in all three parts of the task, however you will only need to modify it yourself in the part \"Learning low-loss valley ...\". In two other parts, you only need to run the correspoding cell.\n",
    "\n",
    "To study the loss surface, we need to be able to reparametrize the weights of the neural network, i. e. compute the weights from some learnable values and backpropagate through the weights. The class below implements this logic. Take a look at this code.\n",
    "\n",
    "This code also allows the visualization of the loss landscape around optima."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Base(nn.Module):\n",
    "    \"\"\"\n",
    "    This class separates logic and parameters of the net:\n",
    "    it stores params in param_net and forwards through net without params (called logic_net)\n",
    "    \n",
    "    Overriding methods,\n",
    "    it is possible to implement diverge weight reparametrization schemes\n",
    "    \n",
    "    terminology:\n",
    "    * weight: tensor that is used during forwad pass through net\n",
    "              it is composed from params, supports and indexes\n",
    "              via parametrization scheme\n",
    "    * param: learnable tensor, torch.nn.Parameter\n",
    "    * support: already learned tensor\n",
    "    * index: random tensor, allows walking through surface\n",
    "    \"\"\"\n",
    "    def __init__(self, net_class, supports_files=[], num_param_nets=1,\n",
    "                 net_args=(), net_kwargs={}):\n",
    "        super(Base, self).__init__()\n",
    "        self.num_param_nets = num_param_nets\n",
    "        self.net_class, self.net_args, self.net_kwargs = \\\n",
    "               net_class, net_args, net_kwargs\n",
    "        self.logic_net = net_class(*net_args, **net_kwargs)\n",
    "        self.clear_net_params(self.logic_net, params_to_buffers=False)\n",
    "        # delete everything learnable from self.logic_net\n",
    "        self.load_supports(supports_files)\n",
    "        self.create_params()\n",
    "        self.create_index()\n",
    "        self.freeze_index = False\n",
    "    \n",
    "    def clear_net_params(self, net, params_to_buffers=False):\n",
    "        \"\"\"\n",
    "        del all params in net (as it shouldn't store params)\n",
    "        and remember what we deleted so we can assign it in the future\n",
    "        \"\"\"\n",
    "        def clear_net_params_rec(net_module, structure):\n",
    "            structure[\"items\"] = []\n",
    "            items = {}\n",
    "            structure[\"children\"] = {}\n",
    "            for name, p in net_module._parameters.items():\n",
    "                structure[\"items\"].append(name)\n",
    "                if p is not None:\n",
    "                    items[name] = p.data if params_to_buffers else \\\n",
    "                                  torch.zeros_like(p.data)\n",
    "                else:\n",
    "                    items[name] = None\n",
    "            net_module._parameters.clear()\n",
    "            for name in items:\n",
    "                net_module.register_buffer(name, items[name])\n",
    "            for mname, child in net_module._modules.items():\n",
    "                structure[\"children\"][mname] = {}\n",
    "                clear_net_params_rec(child, structure[\"children\"][mname])\n",
    "        self.structure = {}\n",
    "        clear_net_params_rec(net, self.structure)\n",
    "\n",
    "    def load_supports(self, supports_files):\n",
    "        \"\"\"\n",
    "        descedants may override this method\n",
    "        \"\"\"\n",
    "        self.support_nets = nn.ModuleList()\n",
    "        for file in supports_files:\n",
    "            if type(file) == str:\n",
    "                support_state_dict = torch.load(file)\n",
    "            else:\n",
    "                support_state_dict = file # state dict is passed instead of file\n",
    "            support_net = self.net_class(*self.net_args, **self.net_kwargs)\n",
    "            support_net.load_state_dict(support_state_dict)\n",
    "            self.clear_net_params(support_net, params_to_buffers=True)\n",
    "            self.support_nets.append(support_net)\n",
    "        \n",
    "    def create_params(self):\n",
    "        \"\"\"\n",
    "        descedants may override this method\n",
    "        \"\"\"\n",
    "        self.param_nets = nn.ModuleList(\\\n",
    "                        [self.net_class(*self.net_args, **self.net_kwargs)\\\n",
    "                         for _ in range(self.num_param_nets)])\n",
    "        # params are stored within param_nets\n",
    "        \n",
    "    def create_index(self):\n",
    "        \"\"\"\n",
    "        descedants may override this method\n",
    "        \"\"\"\n",
    "        self.register_buffer(\"index\", None)\n",
    "    \n",
    "    def process_weights_item(self, param_list, support_list=[]):\n",
    "        \"\"\"\n",
    "        descedants may override this method\n",
    "        \"\"\"\n",
    "        return param_list[0] + 0\n",
    "        \n",
    "    def gen_index(self):\n",
    "        \"\"\"\n",
    "        descedants may override this method\n",
    "        \"\"\"\n",
    "        pass\n",
    "        \n",
    "    def gen_weights(self):\n",
    "        \"\"\"\n",
    "        assignes new generated values to weights of self.logic_net\n",
    "        generated values depend on params, supports and index\n",
    "        descedants may override this method\n",
    "        \"\"\"\n",
    "        def gen_weights_rec(param_nets_modules,\n",
    "                            support_nets_modules,\n",
    "                            logic_net_module,\n",
    "                            structure):\n",
    "            for name in structure[\"items\"]:\n",
    "                param_list = [pnm._parameters[name] for pnm in param_nets_modules]\n",
    "                support_list = [snm._buffers[name] for snm in support_nets_modules]\n",
    "                logic_net_module._buffers[name] = \\\n",
    "                    self.process_weights_item(param_list, support_list) if support_list[0] is not None\\\n",
    "                    else None\n",
    "            for mname, lnm in logic_net_module._modules.items():\n",
    "                pnms = [pnm._modules[mname] for pnm in param_nets_modules]\n",
    "                snms = [snm._modules[mname] for snm in support_nets_modules]\n",
    "                gen_weights_rec(pnms, snms, lnm, structure[\"children\"][mname])\n",
    "        \n",
    "        gen_weights_rec(self.param_nets, \n",
    "                        self.support_nets, \n",
    "                        self.logic_net,\n",
    "                        self.structure)\n",
    "    \n",
    "    def train(self, mode=True):\n",
    "        super(Base, self).train(mode) \n",
    "        if mode:\n",
    "            self.freeze_index = False\n",
    "        \n",
    "    \n",
    "    def forward(self, x):\n",
    "        if not self.freeze_index:\n",
    "            self.gen_index()\n",
    "        self.gen_weights()\n",
    "        return self.logic_net(x)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Class Base only separates parameters from the logic but works as usual neural network. The class below inherits from Base and changes parametrization. Class Segment varies weights along segment \\[a, b\\] where a and b are fixed endpoints:\n",
    "$$w = a  t + b (1-t), \\quad t \\in [0, 1]$$\n",
    "This class doesn't have learnable parameters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Segment(Base):\n",
    "    def process_weights_item(self, param_list, support_list=[]):\n",
    "        t = self.index.item()\n",
    "        return (1-t)*support_list[0]+t*support_list[1]\n",
    "    \n",
    "    def create_index(self):\n",
    "        self.register_buffer(\"index\", torch.zeros(1))\n",
    "    \n",
    "    def gen_index(self):\n",
    "        self.index.uniform_()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing eigenvectors and eigenvalues of the Hessian\n",
    "In this task, you will need 1 trained CNN on MNIST (no saved weights are needed). Train it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cnn = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Sagun et. al, 2017](https://arxiv.org/abs/1706.04454) study the Hessian near the local optima and particularly its eigenvalues and eigenvectors. If the eigenvalue is large, it means that loss changes a lot in the direction of corresponding eigenvector. On the contrary, if eigenvalue is small, the loss is flat in the corresponding direction. Experiments show that near optima there are several large eigenvalues and a lot of small eigenvalues. From this the authors conclude that optima form large connected components at the bottom of the landscape.\n",
    "The authors also note that the number of large eigenvalues usually equals the number of the classes. \n",
    "\n",
    "We suggest you to compute the eigenvalues and eigenvectors of the Hessian near the optima and to check the hypothesis described above. \n",
    "\n",
    "To compute the eigenvalues and eigenvectors, one may use [Lanczos algorithm](https://en.wikipedia.org/wiki/Lanczos_algorithm). This algorithm relies on the computation of Hessian vector product. We will use the implementation of the algorithm from scipy (eigsh). You only have to pass the function that computes the Hessian vector product.\n",
    "\n",
    "This product can be computed in common autograd packages as follows. Let $v$ be a pre-computed numerical vector (such as the gradient). One first computes the scalar $a = \\nabla L^T v$, and then takes the gradient of this expression, resulting in $\\nabla a = Hv$. Let's implement this in Pytorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy.sparse.linalg import LinearOperator, eigsh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def eval_hess_vec_prod(vec, params, net, criterion, dataloader, device):\n",
    "    \"\"\"\n",
    "    Evaluate product of the Hessian of the loss function with a direction vector \"vec\".\n",
    "    The product result is saved in the grad of net.\n",
    "    Args:\n",
    "        vec: a list of tensor with the same dimensions as \"params\".\n",
    "        params: the parameter list of the net.\n",
    "        net: model with trained parameters.\n",
    "        criterion: loss function.\n",
    "        dataloader: dataloader for the dataset.\n",
    "        device: cuda or cpu\n",
    "        \n",
    "    Hint:\n",
    "    1. Perform usual pass through dataloader and compute the loss for each minibatch.\n",
    "    Also, perform backard pass for each mini batch\n",
    "    (NOT cleaning gradient as we will need the graient computed on the FULL dataset)\n",
    "    Use grad_f = torch.autograd.grad(loss, inputs=params, create_graph=True)\n",
    "    2. After that, loop parallelly through grad_f and vec and sum g * v.\n",
    "    3. Finally, perform one more backward pass.\n",
    "    \"\"\"\n",
    "    net.zero_grad() # clears grad for every parameter in the net\n",
    "    ### your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following functions passes your pytorch implementation of hess-vec product to scipy's eigsh."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_hessian_eigs(cnn, dataloader, criterion, device, num_max=20):\n",
    "    params = [p for p in cnn.parameters()]\n",
    "    N = sum(p.numel() for p in params)\n",
    "\n",
    "    def hess_vec_prod(vec):\n",
    "        vec = utils.npvec_to_tensorlist(vec, params, device)\n",
    "        eval_hess_vec_prod(vec, params, cnn, criterion, dataloader, device)\n",
    "        return utils.gradtensor_to_npvec(cnn)\n",
    "\n",
    "    A = LinearOperator((N, N), matvec=hess_vec_prod)\n",
    "    eigvals, eigvecs = eigsh(A, k=num_max, tol=1e-2)\n",
    "    return eigvals, eigvecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "eigvals, eigvecs = get_hessian_eigs(cnn, train_loader, criterion, device, num_max=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's plot the eigenvalues:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.scatter(np.arange(len(eigvals)), eigvals)\n",
    "plt.xlabel(\"Number of eigenvalue\")\n",
    "plt.ylabel(\"Eigenvalue\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also, let's visualize the loss along lines $w_* + v_0 t, \\, t \\in [-0.1, 0.1]$ and $w_* + v_k t, \\, t \\in [-0.1, 0.1], k >> 0$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vecs_dict = {0:eigvecs[:, -1], -1:eigvecs[:, 0]}\n",
    "v = utils.sd2tensor(cnn.state_dict()).cpu().numpy()\n",
    "width = 0.1\n",
    "segments = {}\n",
    "for lab, ev in vecs_dict.items():\n",
    "    l = utils.tensor2sd(torch.from_numpy(v-width*ev), cnn.state_dict())\n",
    "    r = utils.tensor2sd(torch.from_numpy(v+width*ev), cnn.state_dict())\n",
    "    segment = Segment(code.CNN, [l, r], 0).to(device)\n",
    "    segments[lab] = segment\n",
    "index_grid = torch.linspace(0, 1, 21).to(device)\n",
    "res = utils.plot_along_manifold(segments, train_loader, test_loader, code.evaluate_loss_acc, \\\n",
    "                          criterion, device, index_grid, plot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some other works study eigenvalues and eigenvectors along SDG path. \n",
    "[This anonymus work](https://openreview.net/pdf?id=ByeTHsAqtX) find that the subspace spanned by top eigenvectors (corresonding to a few largest eigenvalues) doesn't change a lot after some epoch. Also, they find that the gradient mostly lies in this top subspace. You may conduct corresponding experiments to repeat their results.\n",
    "\n",
    "[Jastrzębski et al, 2019](https://openreview.net/pdf?id=SkgEaj05t7) find that during training, maximum eigenvalue firstly raises and then starts decreasing. It means that at the beginning of the training, the loss curvature gets sharper and sharper, and from some moment SGD starts fluctuating around optima (and actually cannot come closer to the optimum). They relate this moment to the batch size and learning rate. You may also repeat this experiment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning low-loss valley between two independently found optima\n",
    "In this part, you will need 2 CNNs learned on MNIST (with saved weights). Train them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Garipov et al., 2018](https://arxiv.org/pdf/1802.10026.pdf) learn paths between two independetly learned optima. Along these paths, the loss is low and near-constant. This shows that the optima of the loss lie on a connected manifold rather than being separated from each other.\n",
    "\n",
    "The path is learned in a form of simple polychain ($a$ and $b$ represent two learned weight vectors, $\\theta$ is a leanable bench):\n",
    "$$\n",
    "w = \\begin{cases} 2(t \\theta + (0.5-t)a), \\quad  0 \\leq t \\leq 0.5  \\\\ \n",
    "2((t-0.5)b + (1-t)\\theta),\\quad  0.5 < t \\leq 1\\end{cases}\n",
    "$$\n",
    "The model is learned using SGD. When processing 1 minibatch, one firstly samples $t \\in [0, 1]$, then sets weights and performs forward pass through CNN. Gradients are taken w. r. t. $\\theta$. \n",
    "\n",
    "To implement this algortithm, you only need to implment reparametrization scheme in class Polychain. The you may use functions from module  lab_cnn_solution totrain polychain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Polychain(Base):\n",
    "    def process_weights_item(self, param_list, support_list=[]):\n",
    "        ### your code here\n",
    "    \n",
    "    def create_index(self):\n",
    "        ### your code here\n",
    "    \n",
    "    def gen_index(self):\n",
    "        ### your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fn1 = ... # specify endpoint 1\n",
    "fn2 = ... # specify endpoint 2\n",
    "polychain = Polychain(code.CNN, [fn1, fn2]).to(device)\n",
    "segment = Segment(code.CNN, [fn1, fn2]).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will compare learned polychain with straightforward segment $[a, b]$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### your code here\n",
    "### learn polychain as usual model in pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now plot quality along learned polychain $[a, \\theta, b]$ and along segment $[a, b]$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# plot quality along two paths\n",
    "manifolds = {\"segment\":segment, \"polychain\":polychain}\n",
    "index_grid = torch.linspace(0, 1, 21).to(device)\n",
    "res = utils.plot_along_manifold(manifolds, train_loader, test_loader, code.evaluate_loss_acc, \\\n",
    "                          criterion, device, index_grid, plot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As for further experiments, you may try to connect different types of optima (for example, optima learned with small and large batch size) and to check this connectivity hypothesis for other networks / trainng procedures (e. g. use L2 regularization or dropout). [Gotmare et al., 2018](https://arxiv.org/pdf/1806.06977.pdf) conduct such an analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize SGD path using PCA\n",
    "In this part, you will need 1 CNN trained on MNIST with weights saved per epoch. Train it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "path_to_save = ...\n",
    "cnn = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Lorch et al., 2016](https://icmlviz.github.io/icmlviz2016/assets/papers/24.pdf) visualize the trajectory of SGD using PCA. Although training happens in high-dimensional space, its projection onto 2d plane may distinctive properties of the training process. Let's make such a visualization as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fns = [path_to_save+\"/model_ep%d.cpt\"%epoch for epoch in range(num_epochs)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ws = np.array([utils.sd2tensor(torch.load(fn)).cpu().numpy() for fn in fns])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ws.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### your code here\n",
    "# learn PCA on ws and select two principal components\n",
    "mean = ...\n",
    "direction1 = ...\n",
    "direction2 = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### print explained variance ratio\n",
    "### your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Plane2dConnection(Base):\n",
    "    def __init__(self, net_class, mean, direction1, direction2,\n",
    "                 net_args=(), net_kwargs={}):\n",
    "        super(Plane2dConnection, self).__init__(net_class, [mean, direction1, direction2],\\\n",
    "                                                  0, net_args,\\\n",
    "                                                  net_kwargs)\n",
    "    \n",
    "    def process_weights_item(self, param_list, support_list=[]):\n",
    "        t1 = self.index[0].item()\n",
    "        t2 = self.index[1].item()\n",
    "        return support_list[0] + t1*support_list[1] + t2*support_list[2]\n",
    "    \n",
    "    def create_index(self):\n",
    "        self.register_buffer(\"index\", torch.zeros(2))\n",
    "    \n",
    "    def gen_index(self):\n",
    "        self.index.uniform_().mul_(2).add_(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "example_sd = code.CNN().state_dict()\n",
    "plane = Plane2dConnection(code.CNN, \\\n",
    "                      utils.tensor2sd(torch.from_numpy(mean).to(device),\\\n",
    "                                            example_sd),\\\n",
    "                      utils.tensor2sd(torch.from_numpy(direction1).to(device),\\\n",
    "                                            example_sd),\\\n",
    "                      utils.tensor2sd(torch.from_numpy(direction2).to(device),\\\n",
    "                                            example_sd)).to(device)\n",
    "components = np.array([direction1, direction2])\n",
    "utils.plot_2d(components, mean, ws, plane, device, train_loader, test_loader, \\\n",
    "                code.evaluate_loss_acc, criterion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We suggest you to repeat this procedure for different num_epochs because the principal components differ a lot for small and large num_epochs.\n",
    "\n",
    "However, you shouldn't rely so much on this type of analysis. [Antognini and Sohl-Dickstein, 2018](https://arxiv.org/pdf/1806.08805.pdf) theoretically analyse the properties of PCA applied to high-dimensional random walks. They find that explained variance and projection of the trajectory look similar for SGD and random walk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
