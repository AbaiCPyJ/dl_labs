{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most recent version of this notebook is available at https://github.com/nadiinchi/dl_labs/blob/master/lab_attention.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Этот ноутбук содержит практическое введение в механизмы внимания.\n",
    "В ноутбуке рассматривается модельная задача и игрушечная архитектура, поэтому он может быть полностью выполнен на CPU.\n",
    "\n",
    "Рассматриваемая задача - перемножение перестановок.\n",
    "Длина перестановок фиксирована и равна perm_len.\n",
    "Вход - целочисленный вектор длины 2 x perm_len - представляет собой две последовательно записанные перестановки p1 и p2.\n",
    "Произведением p1 и p2 считается такая перестановка p3, что p3[i] = p1[p2[i]].\n",
    "Требуемый выход сети - также целочисленный вектор длины 2 x perm_len, первые perm_len элементов которого равны нулю, а последние являются перестановкой p3.\n",
    "\n",
    "Пример для perm_len = 5:\n",
    "```\n",
    "Вход сети:  3 4 2 1 0 1 3 0 2 4\n",
    "Выход сети: 0 0 0 0 0 4 1 3 2 0\n",
    "Пояснение:  p1 = 3 4 2 1 0,    p2 = 1 3 0 2 4   =>    p3 = 4 1 3 2 0\n",
    "```\n",
    "\n",
    "Теоретически такая задача может быть решена обычным LSTM, который сначала запомнит в скрытом состоянии перестановку p1, а затем проходя по перестановке p2 будет выдавать соответствующие элементы из перестановки p1.\n",
    "На практике, однако, такая модель работает заметно хуже модели с вниманием. Модель с вниманием в явном виде учится проходя по перестановке p2 обращать внимание на нужный элемент перестановки p1 и выдавать его.\n",
    "\n",
    "В задании требуется реализовать и сравнить различные виды внимания, используемые в реальных задачах.\n",
    "Также следует реализовать и использовать кодирование позиций, описанное в статье о трансформерах.\n",
    "Подробнее эти слои и модель описаны ниже."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "import numpy as np\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ниже предлагается реализовать несколько моделей внимания, описанных в разных статьях.\n",
    "В общем случае есть $K$ объектов, на которые можно обращать внимание.\n",
    "Каждый объект характеризуется ключом $k_i$ и значением $v_i$.\n",
    "К слою внимания приходят запросы.\n",
    "Для запроса $q$ слой возвращает взвешенную сумму значений объектов, с весами, пропорциональными степени соответствия ключа запросу:\n",
    "$$w_i = \\frac{\\exp(score(q, k_i))}{\\sum_{j=1}^K\\exp(score(q, k_j))}$$\n",
    "$$a = \\sum_{i=1}^K w_i v_i$$\n",
    "\n",
    "Почти всегда запросы, ключи и значения - вещественные векторы некоторых фиксированных размерностей.\n",
    "В задании предлагается реализовать три вида внимания:\n",
    "+ (опционально) Аддитивное внимание.\n",
    "Задается функцией $score(q, k) = w_3^T \\tanh ( W_1q + W_2k)$, где $W_1, W_2, w_3$ - обучаемые параметры слоя внимания.\n",
    "Для такой функции запрос и ключ могут иметь разную размерность.\n",
    "Матрицы $W_1$ и $W_2$ отображают запрос и ключ в общее скрытое пространство, размерность которого совпадает с размерностью вектора $w_3$.\n",
    "Размерность скрытого пространства может быть выбрана произвольно и является гиперпараметром слоя.\n",
    "Больше информации в статье Bahdanau et al. \"Neural Machine Translation by Jointly Learning to Align and Translate\", 2014.\n",
    "+ Мультипликативное внимание.\n",
    "Задается функцией $score(q, k) = q^Tk$.\n",
    "Для применения такого типа внимания требуется чтобы размерность запроса совпадала с размерностью ключа.\n",
    "Больше информации в статье Luong et al. \"Effective approaches to attention-based neural machine translation\", 2015.\n",
    "+ Отмасштабированное мультипликативное внимание.\n",
    "Задается функцией $score(q, k) = \\frac{q^Tk}{\\sqrt{dim(k)}}$, где $dim(k)$ - размерность ключа (также равная размерности запроса).\n",
    "При обучаемых запросах или ключах такое внимание эквивалентно простому мультипликативному вниманию.\n",
    "Однако сразу после инициализации такое внимание поощряет более сглаженные веса, что смягчает проблему маленьких градиентов при насыщении SoftMax.\n",
    "Больше информации в статье Vaswani et al. \"Attention Is All You Need\", 2017.\n",
    "\n",
    "На практике обычно используют архитектуру в которой ключи и значения объектов совпадают.\n",
    "В прототипах снизу предлагается реализовать именно такую архитектуру.\n",
    "Поскольку ключи и значения совпадают, то они передаются один раз и называются признаками объектов (features) (т. е. $f_i := k_i = v_i$).\n",
    "\n",
    "Также для гибкости интерфейса и ускорения обучения все слои внимания ниже получают для каждого объекта батча по несколько запросов.\n",
    "\n",
    "Класс Attention является родителем классов AdditiveAttention, MultiplicativeAttention и ScaledDotProductAttention.\n",
    "Класс Attention  - абстрактный класс, то есть в качестве слоев  используются только наследники класса Attention, но никогда не он сам.\n",
    "\n",
    "В классе Attention надо реализовать функцию attend, которая принимает для каждого элемента батча набор признаков объектов и набор запросов.\n",
    "Функция attend использует функцию get_scores, реализованную во всех наследниках класса, затем используя полученные значения $score(q, f)$ вычисляет $w$ и $a$.\n",
    "\n",
    "Mask - маска внимания, показывающая для каждого запроса, на какие объекты он не может обращать внимания.\n",
    "Была предложена в статье Vaswani et al. \"Attention Is All You Need\", 2017.\n",
    "Используется для того, чтобы обучаемая модель сохраняла авторегрессионные свойства, то есть чтобы выход слоя для $i$-ой позиции не зависел от значений входа последующих позиций.\n",
    "В функции get_autoregressive_mask надо построить вышеописанную квадратную маску заданного размера.\n",
    "\n",
    "Наиболее численно стабильным способом применения маски в attend будет обращение соответствующих значений score в -float('inf') до применения SoftMax.\n",
    "Альтернативным способом является обнуление весов $w$ в соответствии с маской и их перенормировка, но такой способ менее вычислительно стабилен (подумайте, почему).\n",
    "\n",
    "В каждом из классов AdditiveAttention, MultiplicativeAttention и ScaledDotProductAttention требуется реализовать функцию get_scores, которая для каждого элемента батча для каждого запроса возвращает его похожесть на объекты того же батча.\n",
    "Код get_scores должен быть эквивалетнен следующему:\n",
    "```\n",
    "res = torch.zeros(batch_size, num_queries, num_objects)\n",
    "for i in range(batch_size):\n",
    "    for j in range(num_queries):\n",
    "        for k in range(num_objects):\n",
    "            res[i, j, k] = score(queries[i, j], features[i, k])\n",
    "```\n",
    "Естественно, вышеприведенный код служит лишь иллюстацией, объясняющей размерности аргументов и выхода функции get_scores.\n",
    "Реализованный в задании код должен быть эффективно векторизован.\n",
    "\n",
    "Подсказки по написанию кода:\n",
    "\n",
    "+ В классе AdditiveAttention нужно завести обучаемые параметры $W_1$, $W_2$ и $w_3$.\n",
    "  * Для желающих попрактиковаться в написании своих обучаемых слоев следует помнить, что тензор, содержащийся в наследнике nn.Module не будет перечислен в .parameters() от объекта этого класса, то есть по нему не будет производиться градиентный спуск.\n",
    "Для того, чтобы тензор появился в .parameters(), надо обернуть его в nn.Parameter().\n",
    "Также перед такой оберткой следует инициализировать его, используя какую-нибудь из стандартных инициализаций слоев pytorch.\n",
    "Стоит обратить внимание, что инициализации в pytorch являются in-place, то есть сначала надо завести тензор, а потом передать его в функцию инициализации.\n",
    "  * Для тех, кто не желает практиковаться в написании своих обучаемых слоев, можно вспомнить, что умножение на матрицу эквивалентно применению nn.Linear(.., bias=False).\n",
    "аким образом, функцию внимания можно реализовать с помощью трех линейных слоев, соответствующих матрицам $W_1$, $W_2$, и $w_3$.\n",
    "+ Рекомендуется обратить внимание на функцию torch.bmm, она может пригодиться во многих слоях внимания ниже.\n",
    "+ Для визуализации карты внимания в конце ноутбука надо в функции attend сохранять weights.detach() в self.last_weights. .detach() используется для того, чтобы сохраненные веса не были частью графа вычилений. Не забывайте делать .detach() от отладочного вывода, чтобы граф вычислений не рос сверх необходимого размера."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_autoregressive_mask(size):\n",
    "    \"\"\"\n",
    "    Returns attention mask of given size for autoregressive model.\n",
    "    \"\"\"\n",
    "    # your code here\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def get_scores(self, features, queries):\n",
    "        \"\"\"\n",
    "        features: [batch_size x num_objects x obj_feature_dim]\n",
    "        queries:  [batch_size x num_queries x query_feature_dim]\n",
    "        Returns matrix of scores with shape [batch_size x num_queries x num_objects].\n",
    "        \"\"\"\n",
    "        raise NotImplementedError()                \n",
    "\n",
    "    def attend(self, features, queries, mask=None):\n",
    "        \"\"\"\n",
    "        features:        [batch_size x num_objects x obj_feature_dim]\n",
    "        queries:         [batch_size x num_queries x query_feature_dim]\n",
    "        mask, optional:  [num_queries x num_objects]\n",
    "        Returns matrix of features for queries with shape [batch_size x num_queries x obj_feature_dim].\n",
    "        If mask is not None, sets corresponding to mask weights to zero.\n",
    "        Saves detached weights as self.last_weights for further visualization.\n",
    "        \"\"\"\n",
    "        # your code here\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdditiveAttention(Attention):\n",
    "    \"\"\"\n",
    "    Bahdanau et al. \"Neural Machine Translation by Jointly Learning to Align and Translate\", 2014.\n",
    "    \"\"\"\n",
    "    def __init__(self, obj_feature_dim, query_feature_dim, hidden_dim):\n",
    "        \"\"\"\n",
    "        obj_feature_dim   - dimensionality of attention object features vector\n",
    "        query_feature_dim - dimensionality of attention query vector\n",
    "        hidden_dim        - dimensionality of latent vectors of attention \n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        # your code here\n",
    "\n",
    "    def get_scores(self, features, queries):\n",
    "        \"\"\"\n",
    "        features: [batch_size x num_objects x obj_feature_dim]\n",
    "        queries:  [batch_size x num_queries x query_feature_dim]\n",
    "        Returns matrix of scores with shape [batch_size x num_queries x num_objects].\n",
    "        \"\"\"\n",
    "        # your code here\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiplicativeAttention(Attention):\n",
    "    \"\"\"\n",
    "    Luong et al. \"Effective approaches to attention-based neural machine translation\", 2015.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def get_scores(self, features, queries):\n",
    "        \"\"\"\n",
    "        features: [batch_size x num_objects x feature_dim]\n",
    "        queries:  [batch_size x num_queries x feature_dim]\n",
    "        Returns matrix of scores with shape [batch_size x num_queries x num_objects].\n",
    "        \"\"\"\n",
    "        # your code here\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScaledDotProductAttention(Attention):\n",
    "    \"\"\"\n",
    "    Vaswani et al. \"Attention Is All You Need\", 2017.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def get_scores(self, features, queries):\n",
    "        \"\"\"\n",
    "        features: [batch_size x num_objects x feature_dim]\n",
    "        queries:  [batch_size x num_queries x feature_dim]\n",
    "        Returns matrix of scores with shape [batch_size x num_queries x num_objects].\n",
    "        \"\"\"\n",
    "        # your code here\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# time to check that your attention works\n",
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Функция perm_generator генерирует батч заданного размера объектов для обучения или теста.\n",
    "Для каждого объекта батча случайно равновероятно генерируются перестановки p1 и p2 длины perm_size.\n",
    "Из них формируется из них входная последовательность [p1, p2] и корректный ответ [0, p3] для неё (см. пример выше)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perm_generator(batch_size, perm_size):\n",
    "    \"\"\"\n",
    "    Generates batch of batch_size objects.\n",
    "    Each object consists of two random permutations with length perm_size.\n",
    "    The target for the object is the product of its two permutations.\n",
    "    \"\"\"\n",
    "    # your code here\n",
    "    return objects, correct_answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# time to check your generator\n",
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PositionalEncoder - слой, описанный в Vaswani et al. \"Attention Is All You Need\", 2017.\n",
    "Добавляет к выходу предыдущего слоя эмбеддинги позиций.\n",
    "Чтобы не перевычсилять каждый раз эмбеддинги позиций, получает при создании параметр max_len и предподсчитывает эмбеддинги для позиций с 0 до max_len - 1 включительно.\n",
    "Флаг add указывает, добавлять эмбединги позиций к выходу предыдущего слоя (по умолчанию, при add=True, используется в оригинальной статье) или конкатенировать (add=False).\n",
    "Для выбранной размерности эмбеддингов следует визуализировать эмбеддинги (построить график каждой компоненты эмбеддингов) и подобрать подходящий параметр scale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoder(nn.Module):\n",
    "    def __init__(self, dim, max_len=50, scale=10000.0, add=True):\n",
    "        \"\"\"\n",
    "        Transforms input as described by Vaswani et al. in \"Attention Is All You Need\", 2017.\n",
    "        dim     - dimension of positional embeddings.\n",
    "        max_len - maximal length of sequence, for precomputing\n",
    "        scale   - scale factor for frequency for positional embeddings\n",
    "        add     - boolean, if add is False, concatenate positional embeddings with input instead of adding\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        \n",
    "        self.dim = dim\n",
    "        self.add = add\n",
    "        if add:\n",
    "            self.extra_output_shape = 0\n",
    "        else:\n",
    "            self.extra_output_shape = dim\n",
    "\n",
    "        # your code here\n",
    "               \n",
    "    def forward(self, input):\n",
    "        \"\"\"\n",
    "        input - [batch_size x sequence_len x features_dim]\n",
    "        If self.add is True, self.dim = featurs_dim.\n",
    "        Returns input with added or concatenated positional embeddings (depending on self.add).\n",
    "        \"\"\"\n",
    "        # your code here\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# time to draw positional encoder\n",
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Модель состоит последовательно из следующих слоев:\n",
    "+ Эмбеддинг элементов входной последовательности.\n",
    "+ Эмбеддинг позиций или None, обозначающее отсутствие этого слоя.\n",
    "+ LSTM сеть.\n",
    "Для вычсиления размерности входа LSTM сети можно исползовать размерность первого эмбеддинга и extra_output_shape для эмбеддинга позиций.\n",
    "+ Слой внимания. В качестве запросов получает выходы lSTM сети, в качестве объектов - эмбеддинги последовательности.\n",
    "При установленном флаге autoregressive использует маску внимания для того, чтобы не обращать внимания на элементы последовательности из будущего.\n",
    "+ Логистическая регресиия с perm_len классами, выдающая для каждой позиции парвильный ответ.\n",
    "\n",
    "Обратите внимание, что эта модель не является ни трансформером, ни традиционной сетью использующую LSTM с вниманием.\n",
    "В трансформере используется K-головое внимание, поэлементное преобразование эмбеддингов.\n",
    "В традиционный сетях с LSTM запрос, выданный сетью в предыдущий момент времени, влияет на вход сети в следующий момент времени, поэтому одновременная параллельная обработка всей последовательности невозможна.\n",
    "Также в большинстве сетей используется архитектура энкодер-декодер, где сначала энкодер читает всю входную последовательность и формирует её скрытое представление, а затем декодер выдает выходную последовательность используя это скрытое представление и механизм внимания."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PermMultiplier(nn.Module):\n",
    "    def __init__(self, perm_len, embedding_dim, hidden_dim, attention, pos_enc, autoregressive):\n",
    "        \"\"\"\n",
    "        perm_len       - permutation length (the input is twice longer)\n",
    "        embedding_dim  - dimensionality of integer embeddings\n",
    "        hidden_dim     - dimensionality of LSTM output\n",
    "        attention      - Attention object\n",
    "        pos_enc        - PositionalEncoder object or None\n",
    "        autoregressive - boolean, if True, then model must use autoregressive mask for attention\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.autoregressive = autoregressive\n",
    "        self.perm_len = perm_len\n",
    "        # your code here\n",
    "\n",
    "    def forward(self, input):\n",
    "        \"\"\"\n",
    "        Perform forward pass through layers:\n",
    "        + get embeddings from input sequence (using both embeddings\n",
    "          and positional embeddings if pos_enc is not None)\n",
    "        + run LSTM on embeddings\n",
    "        + use output of LSTM as an attention queries\n",
    "        + attend on the embedded sequence using queries (note autoregressive flag)\n",
    "        + make final linear tranformation to obtain logits\n",
    "        \"\"\"\n",
    "        # your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Есть время писать модели и время обучать их.\n",
    "Сейчас наступило второе.\n",
    "\n",
    "Допишите код для обучения модели ниже.\n",
    "Найдите правильную архитектуру и набор гиперпараметров для модели и метода оптимизации.\n",
    "Утверждается, что для некоторой архитектуры и гиперпараметров модель обучается на CPU за небольшое время."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perm_len = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# time to set up a model\n",
    "# you can check that without pos_enc model doesn't work\n",
    "# not-autoregressive model can be learned easily, but it is less isefull\n",
    "# try to learn autoregressive model if possible\n",
    "pos_enc = PositionalEncoder(?, perm_len * 2, ?, ?)\n",
    "attention = ?\n",
    "model = PermMultiplier(perm_len, ?, ?, attention, pos_enc, ?)\n",
    "if torch.cuda.is_available():\n",
    "    model = model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up optimizer\n",
    "gd = optim.Adam(model.parameters(), lr=?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do optimization\n",
    "avg_loss = None\n",
    "forget = 0.99\n",
    "batch_size = 64\n",
    "iterator = range(?)\n",
    "for i in iterator:\n",
    "    gd.zero_grad()\n",
    "    batch = perm_generator(batch_size, perm_len)\n",
    "    if torch.cuda.is_available():\n",
    "        batch = batch[0].cuda(), batch[1].cuda()\n",
    "    # compute batch loss\n",
    "    # your code here\n",
    "    loss.backward()\n",
    "    if avg_loss is None:\n",
    "        avg_loss = float(loss)\n",
    "    else:\n",
    "        avg_loss = forget * avg_loss + (1 - forget) * float(loss)\n",
    "    descr_str = 'Iteration %05d, loss %.5f.' % (i, avg_loss)\n",
    "    print('\\r', descr_str, end='')\n",
    "    gd.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Отлично, у нас есть какая-то модель.\n",
    "Давайте проверим, как она перемножает две случайных перестановки."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# time to check your model\n",
    "batch = perm_generator(batch_size, perm_len)\n",
    "if torch.cuda.is_available():\n",
    "    batch = batch[0].cuda(), batch[1].cuda()\n",
    "print('Input:\\n', batch[0][:5])\n",
    "print('Output:\\n', ?)\n",
    "print('Correct:\\n', batch[1][:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Одна из важных прикладных свойств внимания - отображать, на что обращает внимание модель.\n",
    "Для этого используются так называемые карты внимания.\n",
    "Используйте поле last_weights слоя Attention, чтобы визуализировать, на какие позиции в каждый момент времени обращала внимание обученная модель для перестановок из батча в ячейке выше.\n",
    "Ожидаемое поведение - в каждый момент времени прохода по перестановке p2 обращать внимание на соответствующий элемент из перестановки p1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize attention map for some object\n",
    "# your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# play with model and learn something new about attention!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
